{
  "metadata": {
    "title": "Operator-Based Generalization Bound for Deep Learning: Insights on Multi-Task Learning",
    "authors": [
      "Mahdi Mohammadigohari",
      "Giuseppe Di Fatta",
      "Giuseppe Nicosia",
      "Panos M. Pardalos"
    ],
    "abstract": "This paper presents novel generalization bounds for vector-valued neural networks and deep kernel methods, focusing on multi-task learning through an operator-theoretic framework. Our key development lies in strategically combining a Koopman-based approach with existing techniques, achieving tighter generalization guarantees compared to traditional norm-based bounds. To mitigate computational challenges associated with Koopman-based methods, we introduce sketching techniques applicable to vector-valued neural networks. These techniques yield excess risk bounds under generic Lipschitz losses, providing performance guarantees for applications including robust and multiple quantile regression. Furthermore, we propose a novel deep learning framework—deep vector-valued reproducing kernel Hilbert spaces (vvRKHS)—leveraging Perron-Frobenius (PF) operators to enhance deep kernel methods. We derive a new Rademacher generalization bound for this framework, explicitly addressing underfitting and overfitting through kernel refinement strategies. This work offers novel insights into the generalization properties of multi-task learning with deep learning architectures, an area that has been relatively unexplored until recent developments."
  },
  "citations_in_text": [
    {
      "marker": "[9]",
      "context_window": "Despite the remarkable empirical successes of deep learning across diverse fields [9, 11], a significant gap remains in our theoretical understanding of their generalization capabilities, especially in multi-task learning scenarios."
    },
    {
      "marker": "[11]",
      "context_window": "Despite the remarkable empirical successes of deep learning across diverse fields [9, 11], a significant gap remains in our theoretical understanding of their generalization capabilities, especially in multi-task learning scenarios."
    },
    {
      "marker": "[31]",
      "context_window": "A key perspective is deep kernel learning [31], where composite functions in reproducing kernel Hilbert spaces (RKHSs) are learned from data, and representer theorems ensure data-dependent solutions [5, 19]."
    },
    {
      "marker": "[5]",
      "context_window": "A key perspective is deep kernel learning [31], where composite functions in reproducing kernel Hilbert spaces (RKHSs) are learned from data, and representer theorems ensure data-dependent solutions [5, 19]."
    },
    {
      "marker": "[19]",
      "context_window": "A key perspective is deep kernel learning [31], where composite functions in reproducing kernel Hilbert spaces (RKHSs) are learned from data, and representer theorems ensure data-dependent solutions [5, 19]."
    },
    {
      "marker": "[7]",
      "context_window": "Relatedly, the neural tangent kernel [7, 17] and convolutional kernel networks [24] enable kernel-based analysis of neural networks, and kernel-inspired regularization has been explored for deep learning [4]."
    },
    {
      "marker": "[17]",
      "context_window": "Relatedly, the neural tangent kernel [7, 17] and convolutional kernel networks [24] enable kernel-based analysis of neural networks, and kernel-inspired regularization has been explored for deep learning [4]."
    },
    {
      "marker": "[24]",
      "context_window": "Relatedly, the neural tangent kernel [7, 17] and convolutional kernel networks [24] enable kernel-based analysis of neural networks, and kernel-inspired regularization has been explored for deep learning [4]."
    },
    {
      "marker": "[4]",
      "context_window": "Relatedly, the neural tangent kernel [7, 17] and convolutional kernel networks [24] enable kernel-based analysis of neural networks, and kernel-inspired regularization has been explored for deep learning [4]."
    },
    {
      "marker": "[29]",
      "context_window": "The generalization properties of both kernel methods and deep neural networks have been extensively investigated, with Rademacher complexity [29] serving as a central tool for bounding generalization errors."
    },
    {
      "marker": "[16]",
      "context_window": "Extensions of this approach have produced generalization bounds for deep kernel methods and vector-valued reproducing kernel Hilbert spaces (vvRKHSs) [16, 19, 35]."
    },
    {
      "marker": "[19]",
      "context_window": "Extensions of this approach have produced generalization bounds for deep kernel methods and vector-valued reproducing kernel Hilbert spaces (vvRKHSs) [16, 19, 35]."
    },
    {
      "marker": "[35]",
      "context_window": "Extensions of this approach have produced generalization bounds for deep kernel methods and vector-valued reproducing kernel Hilbert spaces (vvRKHSs) [16, 19, 35]."
    },
    {
      "marker": "[2]",
      "context_window": "Complementing this, a significant body of research has focused on generalization bounds specific to deep neural networks [2, 13, 14, 15, 18, 30, 36]."
    },
    {
      "marker": "[13]",
      "context_window": "Complementing this, a significant body of research has focused on generalization bounds specific to deep neural networks [2, 13, 14, 15, 18, 30, 36]."
    },
    {
      "marker": "[14]",
      "context_window": "Complementing this, a significant body of research has focused on generalization bounds specific to deep neural networks [2, 13, 14, 15, 18, 30, 36]."
    },
    {
      "marker": "[15]",
      "context_window": "Complementing this, a significant body of research has focused on generalization bounds specific to deep neural networks [2, 13, 14, 15, 18, 30, 36]."
    },
    {
      "marker": "[18]",
      "context_window": "Complementing this, a significant body of research has focused on generalization bounds specific to deep neural networks [2, 13, 14, 15, 18, 30, 36]."
    },
    {
      "marker": "[30]",
      "context_window": "Complementing this, a significant body of research has focused on generalization bounds specific to deep neural networks [2, 13, 14, 15, 18, 30, 36]."
    },
    {
      "marker": "[36]",
      "context_window": "Complementing this, a significant body of research has focused on generalization bounds specific to deep neural networks [2, 13, 14, 15, 18, 30, 36]."
    },
    {
      "marker": "[3]",
      "context_window": "A recent line of research [3, 25] has focused on analyzing generalization performance through the lens of benign overfitting."
    },
    {
      "marker": "[25]",
      "context_window": "A recent line of research [3, 25] has focused on analyzing generalization performance through the lens of benign overfitting."
    },
    {
      "marker": "[28]",
      "context_window": "In [28], we initiated an operator-theoretic approach for analyzing generalization in multi-task deep learning, focusing on full-rank weights and leveraging Koopman operators."
    },
    {
      "marker": "[15]",
      "context_window": "We derived a tighter bound compared to the existing Koopman-based bound [15] by applying a new function space."
    },
    {
      "marker": "[14]",
      "context_window": "Drawing inspiration from Kernel Autoencoders (KAEs) and leveraging the framework of reproducing kernel HilbertC∗-modules—aC ∗-algebra-based extension of RKHS theory—[14] pioneered deep RKHM, a deep architecture built by composing functions within RKHMs using the Perron-Frobenius (PF) operator."
    },
    {
      "marker": "[28]",
      "context_window": "This architecture loosens assumptions on weight matrices and activation functions from previous models in [28, 15] by refining the operator product representation of network layers, resulting in a more compact and expressive structure."
    },
    {
      "marker": "[15]",
      "context_window": "This architecture loosens assumptions on weight matrices and activation functions from previous models in [28, 15] by refining the operator product representation of network layers, resulting in a more compact and expressive structure."
    },
    {
      "marker": "[28]",
      "context_window": "For simplicity, our analysis considers single-output neural networks and RKHSs associated with the one-dimensional Brownian kernel as the function spaces presented in Remark(2)-(i)in [28]."
    },
    {
      "marker": "[30]",
      "context_window": "A further refinement involves combining the Koopman-based framework with established “peeling” techniques [30, 12]."
    },
    {
      "marker": "[12]",
      "context_window": "A further refinement involves combining the Koopman-based framework with established “peeling” techniques [30, 12]."
    },
    {
      "marker": "[6]",
      "context_window": "Following standard practice in kernel methods and deep learning theory, our analysis relies on a set of key assumptions. Specifically, Assumption 1, the attainability condition, is a common starting point in kernel literature [6, 33, 21], positing the existence of a risk minimizer within our hypothesis space."
    },
    {
      "marker": "[33]",
      "context_window": "Following standard practice in kernel methods and deep learning theory, our analysis relies on a set of key assumptions. Specifically, Assumption 1, the attainability condition, is a common starting point in kernel literature [6, 33, 21], positing the existence of a risk minimizer within our hypothesis space."
    },
    {
      "marker": "[21]",
      "context_window": "Following standard practice in kernel methods and deep learning theory, our analysis relies on a set of key assumptions. Specifically, Assumption 1, the attainability condition, is a common starting point in kernel literature [6, 33, 21], positing the existence of a risk minimizer within our hypothesis space."
    },
    {
      "marker": "[10]",
      "context_window": "The existence of such a minimizer is a standard assumption [6, 10, 33, 39] and implies that fHs0 has bounded norm ([10, 33], Remark2)."
    },
    {
      "marker": "[33]",
      "context_window": "The existence of such a minimizer is a standard assumption [6, 10, 33, 39] and implies that fHs0 has bounded norm ([10, 33], Remark2)."
    },
    {
      "marker": "[39]",
      "context_window": "The existence of such a minimizer is a standard assumption [6, 10, 33, 39] and implies that fHs0 has bounded norm ([10, 33], Remark2)."
    },
    {
      "marker": "[10]",
      "context_window": "Following [10, 21], we also assume that the estimators obtained by Empirical Risk Minimization possess bounded norms."
    },
    {
      "marker": "[21]",
      "context_window": "Following [10, 21], we also assume that the estimators obtained by Empirical Risk Minimization possess bounded norms."
    },
    {
      "marker": "[39]",
      "context_window": "The statistical dimension dn is defined in [10] as the minimal index j ∈ {1, . . . , n} such that µj ≤ δ2n, with dn = n if no such j exists."
    },
    {
      "marker": "[10]",
      "context_window": "The statistical dimension dn is defined in [10] as the minimal index j ∈ {1, . . . , n} such that µj ≤ δ2n, with dn = n if no such j exists."
    },
    {
      "marker": "[39]",
      "context_window": "The statistical dimension dn is defined in [10] as the minimal index j ∈ {1, . . . , n} such that µj ≤ δ2n, with dn = n if no such j exists."
    },
    {
      "marker": "[10]",
      "context_window": "The following theorem’s proof directly applies Theorem4from [10], a key theoretical result underpinning this paper."
    },
    {
      "marker": "[10]",
      "context_window": "[10] proposed p-sparsified sketches, demonstrating their k0-satisfiability."
    },
    {
      "marker": "[10]",
      "context_window": "[10] proposed p-sparsified sketches, demonstrating their k0-satisfiability."
    },
    {
      "marker": "[14]",
      "context_window": "Inspired by recent advances in operator-theoretic approaches to deep learning, particularly the framework developed by [14], this section presents a new network architecture designed to capture intricate relationships in multi-task learning."
    },
    {
      "marker": "[14]",
      "context_window": "Inspired by recent advances in operator-theoretic approaches to deep learning, particularly the framework developed by [14], this section presents a new network architecture designed to capture intricate relationships in multi-task learning."
    },
    {
      "marker": "[41]",
      "context_window": "Consequently, the Rademacher complexity can be bounded as bRm n (FL) ≤ √κTr(M1) n sup (fj ∈ Fj)j ∥PfL−1 . . . Pf1 |eV(x)∥∥fL∥HL, (16) where we assume k1 satisfies Assumption 4."
    },
    {
      "marker": "[41]",
      "context_window": "By appropriately refining the vvRKHS, we modulate the trade-off between approximation and sampling errors, mitigating both overfitting and underfitting [41]."
    }
  ],
  "references_list": [
    {
      "ref_id": "[1]",
      "raw_text": "[1] Argyriou, A., Evgeniou, T., Pontil, M.: Multi-task feature learning. In: Advances in Neural Information Processing Systems. vol. 19 (2006)",
      "parsed": {
        "title": "Multi-task feature learning",
        "authors": ["Argyriou, A.", "Evgeniou, T.", "Pontil, M."],
        "year": 2006,
        "venue": "Advances in Neural Information Processing Systems",
        "doi": null
      }
    },
    {
      "ref_id": "[2]",
      "raw_text": "[2] Bartlett, P.L., Foster, D.J., Telgarsky, M.J.: Spectrally-normalized margin bounds for neural networks. In: Proceedings of Advances in Neural Information Processing Systems (NeurIPS). vol. 31 (2017)",
      "parsed": {
        "title": "Spectrally-normalized margin bounds for neural networks",
        "authors": ["Bartlett, P.L.", "Foster, D.J.", "Telgarsky, M.J."],
        "year": 2017,
        "venue": "Proceedings of Advances in Neural Information Processing Systems (NeurIPS)",
        "doi": null
      }
    },
    {
      "ref_id": "[3]",
      "raw_text": "[3] Bartlett, P.L., Long, P.M., Lugosi, G., Tsigler, A.: Benign overfitting in linear regression. Proceedings of the National Academy of Sciences 117(48), 30063–30070 (2020)",
      "parsed": {
        "title": "Benign overfitting in linear regression",
        "authors": ["Bartlett, P.L.", "Long, P.M.", "Lugosi, G.", "Tsigler, A."],
        "year": 2020,
        "venue": "Proceedings of the National Academy of Sciences",
        "doi": null
      }
    },
    {
      "ref_id": "[4]",
      "raw_text": "[4] Bietti, A., Mialon, G., Chen, D., Mairal, J.: A kernel perspective for regularizing deep neural networks. In: In Proceedings of the 36th International Conference on Machine Learning (ICML) (2019)",
      "parsed": {
        "title": "A kernel perspective for regularizing deep neural networks",
        "authors": ["Bietti, A.", "Mialon, G.", "Chen, D.", "Mairal, J."],
        "year": 2019,
        "venue": "Proceedings of the 36th International Conference on Machine Learning (ICML)",
        "doi": null
      }
    },
    {
      "ref_id": "[5]",
      "raw_text": "[5] Bohn, B., Griebel, M., Rieger, C.: A representer theorem for deep kernel learning. Journal of Machine Learning Research 20(64), 1–32 (2019)",
      "parsed": {
        "title": "A representer theorem for deep kernel learning",
        "authors": ["Bohn, B.", "Griebel, M.", "Rieger, C."],
        "year": 2019,
        "venue": "Journal of Machine Learning Research",
        "doi": null
      }
    },
    {
      "ref_id": "[6]",
      "raw_text": "[6] Caponnetto, A., Vito, E.D.: Optimal rates for the regularized least-squares algorithm. Foundations of Computational Mathematics 7(3), 331–368 (2007)",
      "parsed": {
        "title": "Optimal rates for the regularized least-squares algorithm",
        "authors": ["Caponnetto, A.", "Vito, E.D."],
        "year": 2007,
        "venue": "Foundations of Computational Mathematics",
        "doi": null
      }
    },
    {
      "ref_id": "[7]",
      "raw_text": "[7] Chen, L., Xu, S.: Deep neural tangent kernel and laplace kernel have the same RKHS. In: In Proceedings of the 9th International Conference on Learning Representations (ICLR) (2021)",
      "parsed": {
        "title": "Deep neural tangent kernel and laplace kernel have the same RKHS",
        "authors": ["Chen, L.", "Xu, S."],
        "year": 2021,
        "venue": "Proceedings of the 9th International Conference on Learning Representations (ICLR)",
        "doi": null
      }
    },
    {
      "ref_id": "[8]",
      "raw_text": "[8] Collins, L., Hassani, H., Soltanolkotabi, M., Mokhtari, A., Shakkottai, S.: Provable multi-task representation learning by two-layer relu neural networks. In: Proceedings of the Forty-first International Conference on Machine Learning (2024)",
      "parsed": {
        "title": "Provable multi-task representation learning by two-layer relu neural networks",
        "authors": ["Collins, L.", "Hassani, H.", "Soltanolkotabi, M.", "Mokhtari, A.", "Shakkottai, S."],
        "year": 2024,
        "venue": "Proceedings of the Forty-first International Conference on Machine Learning",
        "doi": null
      }
    },
    {
      "ref_id": "[9]",
      "raw_text": "[9] Crawshaw, M.: Multi-task learning with deep neural networks: A survey. arXiv preprint arXiv:2009.09796 (2020)",
      "parsed": {
        "title": "Multi-task learning with deep neural networks: A survey",
        "authors": ["Crawshaw, M."],
        "year": 2020,
        "venue": "arXiv preprint",
        "doi": null
      }
    },
    {
      "ref_id": "[10]",
      "raw_text": "[10] El Ahmad, T., Laforgue, P., d’Alché Buc, F.: Fast kernel methods for generic Lipschitz losses via p-sparsified sketches. Transactions on Machine Learning Research (2023)",
      "parsed": {
        "title": "Fast kernel methods for generic Lipschitz losses via p-sparsified sketches",
        "authors": ["El Ahmad, T.", "Laforgue, P.", "d’Alché Buc, F."],
        "year": 2023,
        "venue": "Transactions on Machine Learning Research",
        "doi": null
      }
    },
    {
      "ref_id": "[11]",
      "raw_text": "[11] Fatta, G.D., Nicosia, G., Ojha, V., Pardalos, P.: Multi-task deep learning as multi-objective optimization. In: Encyclopedia of Optimization, pp. 1–10. Springer (2023)",
      "parsed": {
        "title": "Multi-task deep learning as multi-objective optimization",
        "authors": ["Fatta, G.D.", "Nicosia, G.", "Ojha, V.", "Pardalos, P."],
        "year": 2023,
        "venue": "Encyclopedia of Optimization",
        "doi": null
      }
    },
    {
      "ref_id": "[12]",
      "raw_text": "[12] Golowich, N., Rakhlin, A., Shamir, O.: Size-independent sample complexity of neural networks. In: Proceedings of the 2018 Conference On Learning Theory (COLT) (2018)",
      "parsed": {
        "title": "Size-independent sample complexity of neural networks",
        "authors": ["Golowich, N.", "Rakhlin, A.", "Shamir, O."],
        "year": 2018,
        "venue": "Proceedings of the 2018 Conference On Learning Theory (COLT)",
        "doi": null
      }
    },
    {
      "ref_id": "[13]",
      "raw_text": "[13] Golowich, N., Rakhlin, A., Shamir, O.: Size-independent sample complexity of neural networks. Information and Inference: A Journal of the IMA 9(2), 473–504 (6 2020)",
      "parsed": {
        "title": "Size-independent sample complexity of neural networks",
        "authors": ["Golowich, N.", "Rakhlin, A.", "Shamir, O."],
        "year": 2020,
        "venue": "Information and Inference: A Journal of the IMA",
        "doi": null
      }
    },
    {
      "ref_id": "[14]",
      "raw_text": "[14] Hashimoto, Y., Ikeda, M., Kadri, H.: Deep learning with kernels through rkhm and the perron-frobenius operator. In: Advances in Neural Information Processing Systems. vol. 36, pp. 50677–50696 (2023)",
      "parsed": {
        "title": "Deep learning with kernels through rkhm and the perron-frobenius operator",
        "authors": ["Hashimoto, Y.", "Ikeda, M.", "Kadri, H."],
        "year": 2023,
        "venue": "Advances in Neural Information Processing Systems",
        "doi": null
      }
    },
    {
      "ref_id": "[15]",
      "raw_text": "[15] Hashimoto, Y., Sonoda, S., Ishikawa, I., Nitanda, A., Suzuki, T.: Koopman-based generalization bound: New aspect for full-rank weights. In: The Twelfth International Conference on Learning Representations (2024), https://openreview.net/forum?id=JN7TcCm9LF",
      "parsed": {
        "title": "Koopman-based generalization bound: New aspect for full-rank weights",
        "authors": ["Hashimoto, Y.", "Sonoda, S.", "Ishikawa, I.", "Nitanda, A.", "Suzuki, T."],
        "year": 2024,
        "venue": "The Twelfth International Conference on Learning Representations",
        "doi": null
      }
    },
    {
      "ref_id": "[16]",
      "raw_text": "[16] Huusari, R., Kadri, H.: Entangled kernels - beyond separability. Journal of Machine Learning Research 22(24), 1–40 (2021)",
      "parsed": {
        "title": "Entangled kernels - beyond separability",
        "authors": ["Huusari, R.", "Kadri, H."],
        "year": 2021,
        "venue": "Journal of Machine Learning Research",
        "doi": null
      }
    },
    {
      "ref_id": "[17]",
      "raw_text": "[17] Jacot, A., Gabriel, F., Hongler, C.: Neural tangent kernel: Convergence and generalization in neural networks. In: In Proceedings of Advances in Neural Information Processing Systems (NeurIPS). vol. 31 (2018)",
      "parsed": {
        "title": "Neural tangent kernel: Convergence and generalization in neural networks",
        "authors": ["Jacot, A.", "Gabriel, F.", "Hongler, C."],
        "year": 2018,
        "venue": "Proceedings of Advances in Neural Information Processing Systems (NeurIPS)",
        "doi": null
      }
    },
    {
      "ref_id": "[18]",
      "raw_text": "[18] Ju, H., Li, D., Zhang, H.R.: Robust fine-tuning of deep neural networks with Hessian-based generalization guarantees. In: Proceedings of the 39th International Conference on Machine Learning (ICML) (2022)",
      "parsed": {
        "title": "Robust fine-tuning of deep neural networks with Hessian-based generalization guarantees",
        "authors": ["Ju, H.", "Li, D.", "Zhang, H.R."],
        "year": 2022,
        "venue": "Proceedings of the 39th International Conference on Machine Learning (ICML)",
        "doi": null
      }
    },
    {
      "ref_id": "[19]",
      "raw_text": "[19] Laforgue, P., Clémençon, S., d’Alché Buc, F.: Autoencoding any data through kernel autoencoders. In: Proceedings of the 22nd International Conference on Artificial Intelligence and Statistics (AISTATS) (2019)",
      "parsed": {
        "title": "Autoencoding any data through kernel autoencoders",
        "authors": ["Laforgue, P.", "Clémençon, S.", "d’Alché Buc, F."],
        "year": 2019,
        "venue": "Proceedings of the 22nd International Conference on Artificial Intelligence and Statistics (AISTATS)",
        "doi": null
      }
    },
    {
      "ref_id": "[20]",
      "raw_text": "[20] Li, Z., Meunier, D., Mollenhauer, M., Gretton, A.: Towards optimal sobolev norm rates for the vector-valued regularized least-squares algorithm. Journal of Machine Learning Research 25(181), 1–51 (2024), http://jmlr.org/papers/v25/23-1663.html",
      "parsed": {
        "title": "Towards optimal sobolev norm rates for the vector-valued regularized least-squares algorithm",
        "authors": ["Li, Z.", "Meunier, D.", "Mollenhauer, M.", "Gretton, A."],
        "year": 2024,
        "venue": "Journal of Machine Learning Research",
        "doi": null
      }
    },
    {
      "ref_id": "[21]",
      "raw_text": "[21] Li, Z., Ton, J.F., Oglic, D., Sejdinovic, D.: Towards a unified analysis of random fourier features. Journal of Machine Learning Research 22(108), 1–51 (2021)",
      "parsed": {
        "title": "Towards a unified analysis of random fourier features",
        "authors": ["Li, Z.", "Ton, J.F.", "Oglic, D.", "Sejdinovic, D."],
        "year": 2021,
        "venue": "Journal of Machine Learning Research",
        "doi": null
      }
    },
    {
      "ref_id": "[22]",
      "raw_text": "[22] Lindsey, J.W., Lippl, S.: Implicit regularization of multi-task learning and finetuning in overparameterized neural networks. arXiv preprint arXiv:2310.02396 (2023)",
      "parsed": {
        "title": "Implicit regularization of multi-task learning and finetuning in overparameterized neural networks",
        "authors": ["Lindsey, J.W.", "Lippl, S."],
        "year": 2023,
        "venue": "arXiv preprint",
        "doi": null
      }
    },
    {
      "ref_id": "[23]",
      "raw_text": "[23] Mahoney, M.W., et al.: Randomized algorithms for matrices and data. Foundations and Trends® in Machine Learning 3(2), 123–224 (2011)",
      "parsed": {
        "title": "Randomized algorithms for matrices and data",
        "authors": ["Mahoney, M.W."],
        "year": 2011,
        "venue": "Foundations and Trends® in Machine Learning",
        "doi": null
      }
    },
    {
      "ref_id": "[24]",
      "raw_text": "[24] Mairal, J., Koniusz, P., Harchaoui, Z., Schmid, C.: Convolutional kernel networks. In: In Proceedings of the Advances in Neural Information Processing Systems (NIPS). vol. 27 (2014)",
      "parsed": {
        "title": "Convolutional kernel networks",
        "authors": ["Mairal, J.", "Koniusz, P.", "Harchaoui, Z.", "Schmid, C."],
        "year": 2014,
        "venue": "Proceedings of the Advances in Neural Information Processing Systems (NIPS)",
        "doi": null
      }
    },
    {
      "ref_id": "[25]",
      "raw_text": "[25] Mallinar, N.R., Simon, J.B., Abedsoltan, A., Pandit, P., Belkin, M., Nakkiran, P.: Benign, tempered, or catastrophic: Toward a refined taxonomy of overfitting. In: In Proceedings of Advances in Neural Information Processing Systems (NeurIPS). vol. 37 (2022)",
      "parsed": {
        "title": "Benign, tempered, or catastrophic: Toward a refined taxonomy of overfitting",
        "authors": ["Mallinar, N.R.", "Simon, J.B.", "Abedsoltan, A.", "Pandit, P.", "Belkin, M.", "Nakkiran, P."],
        "year": 2022,
        "venue": "Proceedings of Advances in Neural Information Processing Systems (NeurIPS)",
        "doi": null
      }
    },
    {
      "ref_id": "[26]",
      "raw_text": "[26] Maurer, A.: Bounds for linear multi-task learning. The Journal of Machine Learning Research 7, 117–139 (2006)",
      "parsed": {
        "title": "Bounds for linear multi-task learning",
        "authors": ["Maurer, A."],
        "year": 2006,
        "venue": "The Journal of Machine Learning Research",
        "doi": null
      }
    },
    {
      "ref_id": "[27]",
      "raw_text": "[27] Micchelli, C.A., Pontil, M.: On learning vector-valued functions. Neural Computation 17(1), 177–204 (2005)",
      "parsed": {
        "title": "On learning vector-valued functions",
        "authors": ["Micchelli, C.A.", "Pontil, M."],
        "year": 2005,
        "venue": "Neural Computation",
        "doi": null
      }
    },
    {
      "ref_id": "[28]",
      "raw_text": "[28] Mohammadigohari, M., Di Fatta, G., Nicosia, G., Pardalos, P.: On the koopman-based generalization bounds for multi-task deep learning. In: Nicosia, G., et al. (eds.) Proceedings of the International Conference on Learning and Discovery (LOD). Lecture Notes in Computer Science, vol. To be added, p. To be added. Springer, Cham (2025), accepted",
      "parsed": {
        "title": "On the koopman-based generalization bounds for multi-task deep learning",
        "authors": ["Mohammadigohari, M.", "Di Fatta, G.", "Nicosia, G.", "Pardalos, P."],
        "year": 2025,
        "venue": "Proceedings of the International Conference on Learning and Discovery (LOD). Lecture Notes in Computer Science",
        "doi": null
      }
    },
    {
      "ref_id": "[29]",
      "raw_text": "[29] Mohri, M., Rostamizadeh, A., Talwalkar, A.: Foundations of Machine Learning. MIT Press, Cambridge, MA (2018)",
      "parsed": {
        "title": "Foundations of Machine Learning",
        "authors": ["Mohri, M.", "Rostamizadeh, A.", "Talwalkar, A."],
        "year": 2018,
        "venue": "MIT Press, Cambridge, MA",
        "doi": null
      }
    },
    {
      "ref_id": "[30]",
      "raw_text": "[30] Neyshabur, B., Tomioka, R., Srebro, N.: Norm-based capacity control in neural networks. In: Proceedings of the 2015 Conference on Learning Theory (COLT) (2015)",
      "parsed": {
        "title": "Norm-based capacity control in neural networks",
        "authors": ["Neyshabur, B.", "Tomioka, R.", "Srebro, N."],
        "year": 2015,
        "venue": "Proceedings of the 2015 Conference on Learning Theory (COLT)",
        "doi": null
      }
    },
    {
      "ref_id": "[31]",
      "raw_text": "[31] Ober, S.W., Rasmussen, C.E., van der Wilk, M.: The promises and pitfalls of deep kernel learning. In: Proceedings of the 37th Conference on Uncertainty in Artificial Intelligence (UAI) (2021)",
      "parsed": {
        "title": "The promises and pitfalls of deep kernel learning",
        "authors": ["Ober, S.W.", "Rasmussen, C.E.", "van der Wilk, M."],
        "year": 2021,
        "venue": "Proceedings of the 37th Conference on Uncertainty in Artificial Intelligence (UAI)",
        "doi": null
      }
    },
    {
      "ref_id": "[32]",
      "raw_text": "[32] Pontil, M., Maurer, A.: Excess risk bounds for multitask learning with trace norm regularization. In: Proceedings of the Conference on Learning Theory. pp. 55–76. PMLR (2013)",
      "parsed": {
        "title": "Excess risk bounds for multitask learning with trace norm regularization",
        "authors": ["Pontil, M.", "Maurer, A."],
        "year": 2013,
        "venue": "Proceedings of the Conference on Learning Theory",
        "doi": null
      }
    },
    {
      "ref_id": "[33]",
      "raw_text": "[33] Rudi, A., Rosasco, L.: Generalization properties of learning with random features. In: Advances in Neural Information Processing Systems (NeurIPS). pp. 3215–3225 (2017)",
      "parsed": {
        "title": "Generalization properties of learning with random features",
        "authors": ["Rudi, A.", "Rosasco, L."],
        "year": 2017,
        "venue": "Advances in Neural Information Processing Systems (NeurIPS)",
        "doi": null
      }
    },
    {
      "ref_id": "[34]",
      "raw_text": "[34] Shenouda, J., Parhi, R., Lee, K., Nowak, R.D.: Variation spaces for multi-output neural networks: Insights on multi-task learning and network compression. Journal of Machine Learning Research 25(231), 1–40 (2024)",
      "parsed": {
        "title": "Variation spaces for multi-output neural networks: Insights on multi-task learning and network compression",
        "authors": ["Shenouda, J.", "Parhi, R.", "Lee, K.", "Nowak, R.D."],
        "year": 2024,
        "venue": "Journal of Machine Learning Research",
        "doi": null
      }
    },
    {
      "ref_id": "[35]",
      "raw_text": "[35] Sindhwani, V., Minh, H.Q., Lozano, A.C.: Scalable matrix-valued kernel learning for high-dimensional nonlinear multivariate regression and granger causality. In: Proceedings of the 29th Conference on Uncertainty in Artificial Intelligence (UAI) (2013)",
      "parsed": {
        "title": "Scalable matrix-valued kernel learning for high-dimensional nonlinear multivariate regression and granger causality",
        "authors": ["Sindhwani, V.", "Minh, H.Q.", "Lozano, A.C."],
        "year": 2013,
        "venue": "Proceedings of the 29th Conference on Uncertainty in Artificial Intelligence (UAI)",
        "doi": null
      }
    },
    {
      "ref_id": "[36]",
      "raw_text": "[36] Suzuki, T., Abe, H., Nishimura, T.: Compression based bound for non-compressed network: unified generalization error analysis of large compressible deep neural network. In: Proceedings of the 8th International Conference on Learning Representations (ICLR) (2020)",
      "parsed": {
        "title": "Compression based bound for non-compressed network: unified generalization error analysis of large compressible deep neural network",
        "authors": ["Suzuki, T.", "Abe, H.", "Nishimura, T."],
        "year": 2020,
        "venue": "Proceedings of the 8th International Conference on Learning Representations (ICLR)",
        "doi": null
      }
    },
    {
      "ref_id": "[37]",
      "raw_text": "[37] Wittwar, D.: Approximation with matrix-valued kernels and highly effective error estimators for reduced basis approximations. Ph.D. thesis, Universität Stuttgart, Stuttgart, Germany (April 2022)",
      "parsed": {
        "title": "Approximation with matrix-valued kernels and highly effective error estimators for reduced basis approximations",
        "authors": ["Wittwar, D."],
        "year": 2022,
        "venue": "Ph.D. thesis, Universität Stuttgart, Stuttgart, Germany",
        "doi": null
      }
    },
    {
      "ref_id": "[38]",
      "raw_text": "[38] Woodruff, D.P.: Sketching as a tool for numerical linear algebra. Foundations and Trends® in Theoretical Computer Science 10(1-2), 1–157 (2014)",
      "parsed": {
        "title": "Sketching as a tool for numerical linear algebra",
        "authors": ["Woodruff, D.P."],
        "year": 2014,
        "venue": "Foundations and Trends® in Theoretical Computer Science",
        "doi": null
      }
    },
    {
      "ref_id": "[39]",
      "raw_text": "[39] Yang, Y., Pilanci, M., Wainwright, M.J., others if applicable], .: Randomized sketches for kernels: Fast and optimal nonparametric regression. The Annals of Statistics 45(3), 991–1023 (2017)",
      "parsed": {
        "title": "Randomized sketches for kernels: Fast and optimal nonparametric regression",
        "authors": ["Yang, Y.", "Pilanci, M.", "Wainwright, M.J."],
        "year": 2017,
        "venue": "The Annals of Statistics",
        "doi": null
      }
    },
    {
      "ref_id": "[40]",
      "raw_text": "[40] Yousefi, N., Lei, Y., Kloft, M., Mollaghasemi, M., Anagnostopoulos, G.C.: Local rademacher complexity-based learning guarantees for multi-task learning. The Journal of Machine Learning Research 19(1), 1385–1431 (2018)",
      "parsed": {
        "title": "Local rademacher complexity-based learning guarantees for multi-task learning",
        "authors": ["Yousefi, N.", "Lei, Y.", "Kloft, M.", "Mollaghasemi, M.", "Anagnostopoulos, G.C."],
        "year": 2018,
        "venue": "The Journal of Machine Learning Research",
        "doi": null
      }
    },
    {
      "ref_id": "[41]",
      "raw_text": "[41] Zhang, H., Xu, Y., Zhang, Q.: Refinement of operator-valued reproducing kernels. Journal of Machine Learning Research 13(4), 91–136 (2012), http://jmlr.org/papers/v13/zhang12a.html",
      "parsed": {
        "title": "Refinement of operator-valued reproducing kernels",
        "authors": ["Zhang, H.", "Xu, Y.", "Zhang, Q."],
        "year": 2012,
        "venue": "Journal of Machine Learning Research",
        "doi": null
      }
    }
  ]
}