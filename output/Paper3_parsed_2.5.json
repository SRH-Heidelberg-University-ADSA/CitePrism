{
  "metadata": {
    "title": "Operator-Based Generalization Bound for Deep Learning: Insights on Multi-Task Learning",
    "doi": null,
    "authors": [
      "Mahdi Mohammadigohari",
      "Giuseppe Di Fatta",
      "Giuseppe Nicosia",
      "Panos M. Pardalos"
    ],
    "abstract": "This paper presents novel generalization bounds for vector-valued neural networks and deep kernel methods, focusing on multi-task learning through an operator-theoretic framework. Our key development lies in strategically combining a Koopman-based approach with existing techniques, achieving tighter generalization guarantees compared to traditional norm-based bounds. To mitigate computational challenges associated with Koopman-based methods, we introduce sketching techniques applicable to vector-valued neural networks. These techniques yield excess risk bounds under generic Lipschitz losses, providing performance guarantees for applications including robust and multiple quantile regression. Furthermore, we propose a novel deep learning framework—deep vector-valued reproducing kernel Hilbert spaces (vvRKHS)—leveraging Perron-Frobenius (PF) operators to enhance deep kernel methods. We derive a new Rademacher generalization bound for this framework, explicitly addressing underfitting and overfitting through kernel refinement strategies. This work offers novel insights into the generalization properties of multi-task learning with deep learning architectures, an area that has been relatively unexplored until recent developments."
  },
  "citations_in_text": [
    {
      "marker": "[9, 11]",
      "context_window": "Despite the remarkable empirical successes of deep learning across diverse fields [9, 11], a significant gap remains in our theoretical understanding of their generalization capabilities, especially in multi-task learning scenarios. This work is specifically motivated by the need to overcome the limitations of existing generalization bounds in capturing the performance of deep learning models and deep kernel methods in multi-task settings, an area where a deeper understanding of network structure, operator theory, and adaptive kernel refinement strategies is essential for realizing robust and reliable performance."
    },
    {
      "marker": "[31]",
      "context_window": "A key perspective is deep kernel learning [31], where composite functions in reproducing kernel Hilbert spaces (RKHSs) are learned from data, and representer theorems ensure data-dependent solutions [5, 19]."
    },
    {
      "marker": "[5, 19]",
      "context_window": "A key perspective is deep kernel learning [31], where composite functions in reproducing kernel Hilbert spaces (RKHSs) are learned from data, and representer theorems ensure data-dependent solutions [5, 19]. This blends deep neural network flexibility with the theoretical rigor of kernel methods."
    },
    {
      "marker": "[7, 17]",
      "context_window": "This blends deep neural network flexibility with the theoretical rigor of kernel methods. Relatedly, the neural tangent kernel [7, 17] and convolutional kernel networks [24] enable kernel-based analysis of neural networks, and kernel-inspired regularization has been explored for deep learning [4]."
    },
    {
      "marker": "[24]",
      "context_window": "Relatedly, the neural tangent kernel [7, 17] and convolutional kernel networks [24] enable kernel-based analysis of neural networks, and kernel-inspired regularization has been explored for deep learning [4]."
    },
    {
      "marker": "[4]",
      "context_window": "Relatedly, the neural tangent kernel [7, 17] and convolutional kernel networks [24] enable kernel-based analysis of neural networks, and kernel-inspired regularization has been explored for deep learning [4]."
    },
    {
      "marker": "[29]",
      "context_window": "The generalization properties of both kernel methods and deep neural networks have been extensively investigated, with Rademacher complexity [29] serving as a central tool for bounding generalization errors. For kernel methods, Rademacher complexity bounds can be derived directly through the reproducing property."
    },
    {
      "marker": "[16, 19, 35]",
      "context_window": "For kernel methods, Rademacher complexity bounds can be derived directly through the reproducing property. Extensions of this approach have produced generalization bounds for deep kernel methods and vector-valued reproducing kernel Hilbert spaces (vvRKHSs) [16, 19, 35]. Complementing this, a significant body of research has focused on generalization bounds specific to deep neural networks [2, 13, 14, 15, 18, 30, 36]."
    },
    {
      "marker": "[2, 13, 14, 15, 18, 30, 36]",
      "context_window": "Extensions of this approach have produced generalization bounds for deep kernel methods and vector-valued reproducing kernel Hilbert spaces (vvRKHSs) [16, 19, 35]. Complementing this, a significant body of research has focused on generalization bounds specific to deep neural networks [2, 13, 14, 15, 18, 30, 36]. A recent line of research [3, 25] has focused on analyzing generalization performance through the lens of benign overfitting."
    },
    {
      "marker": "[3, 25]",
      "context_window": "A recent line of research [3, 25] has focused on analyzing generalization performance through the lens of benign overfitting. Despite these advances, a dedicated generalization analysis method for deep vector-valued kernel learning and multi-output deep neural networks remains underdeveloped, hindering the derivation of tighter, more meaningful generalization error bounds. In the general vector-valued setting, these spaces exhibit several intriguing properties that shed light on multi-task learning with neural networks; such insight cannot be achieved by the former studies."
    },
    {
      "marker": "[28]",
      "context_window": "This paper introduces novel generalization bounds for vector-valued NNs in the multi-task learning setting. In [28], we initiated an operator-theoretic approach for analyzing generalization in multi-task deep learning, focusing on full-rank weights and leveraging Koopman operators. We derived a tighter bound compared to the existing Koopman-based bound [15] by applying a new function space."
    },
    {
      "marker": "[15]",
      "context_window": "In [28], we initiated an operator-theoretic approach for analyzing generalization in multi-task deep learning, focusing on full-rank weights and leveraging Koopman operators. We derived a tighter bound compared to the existing Koopman-based bound [15] by applying a new function space. In this work, the first part continues this line by introducing input space sketching—a dimensionality reduction technique to enhance practical applicability in large-scale settings."
    },
    {
      "marker": "[14]",
      "context_window": "Drawing inspiration from Kernel Autoencoders (KAEs) and leveraging the framework of reproducing kernel HilbertC∗-modules—aC ∗-algebra-based extension of RKHS theory—[14] pioneered deep RKHM, a deep architecture built by composing functions within RKHMs using the Perron-Frobenius (PF) operator. We strategically combine this approach with existing techniques and, in the second part, we presents a new network architecture, deep vvRKHS, constructed through using PF operators."
    },
    {
      "marker": "[28, 15]",
      "context_window": "This architecture loosens assumptions on weight matrices and activation functions from previous models in [28, 15] by refining the operator product representation of network layers, resulting in a more compact and expressive structure."
    },
    {
      "marker": "[1]",
      "context_window": "Multi-TaskLearning:Theadvantagesofmulti-tasklearninghavebeenextensivelyexplored inthemachinelearningliterature[1]. Recenttheoreticalresearch has focused on properties of multi-task neural networks and how shared repre-sentations and task relationships influence generalization [22, 8, 34]."
    },
    {
      "marker": "[22, 8, 34]",
      "context_window": "Recent theoretical research has focused on properties of multi-task neural networks and how shared repre-sentations and task relationships influence generalization [22, 8, 34]. Initial generalization results in multi-task learning include Rademacher complexity-based bounds for linear classification [26], subsequently refined to derive risk estimates in trace norm regularized models [32]."
    },
    {
      "marker": "[26]",
      "context_window": "Initial generalization results in multi-task learning include Rademacher complexity-based bounds for linear classification [26], subsequently refined to derive risk estimates in trace norm regularized models [32]. More recently, sharper risk bounds lever-aging local Rademacher complexity analysed the role of common regularization strategies [40]."
    },
    {
      "marker": "[32]",
      "context_window": "Initial generalization results in multi-task learning include Rademacher complexity-based bounds for linear classification [26], subsequently refined to derive risk estimates in trace norm regularized models [32]. More recently, sharper risk bounds lever-aging local Rademacher complexity analysed the role of common regularization strategies [40]."
    },
    {
      "marker": "[40]",
      "context_window": "More recently, sharper risk bounds lever-aging local Rademacher complexity analysed the role of common regularization strategies [40]. To address the current scarcity of multi-task deep learning the-ories, this work introduces a novel framework to analyzing the generalization characteristics of functions learned by vector-valued deep learning architectures with use of transfer operators, providing new insights into multi-task learning with neural networks."
    },
    {
      "marker": "[23, 38, 39]",
      "context_window": "Sketching:To alleviate the computational demands of kernel methods, sketch-ing techniques have emerged as a powerful tool for approximating kernel matri-ces and reducing memory requirements [23, 38, 39]. By employing randomized linear projections, sketching can enable near-optimal nonparametric regression [39] and provide efficient solutions for large-scale problems."
    },
    {
      "marker": "[39]",
      "context_window": "By employing randomized linear projections, sketching can enable near-optimal nonparametric regression [39] and provide efficient solutions for large-scale problems. Approaches such as Random Fourier Features [21] have proven effective, while recent work onp-sparsifiedsketches[10]demonstratesthatstructuredsparsitycanofferbothcom-putational advantages and strong theoretical guarantees, extending the range of applicability for generic Lipschitz losses."
    },
    {
      "marker": "[21]",
      "context_window": "Approaches such as Random Fourier Features [21] have proven effective, while recent work onp-sparsifiedsketches[10]demonstratesthatstructuredsparsitycanofferbothcom-putational advantages and strong theoretical guarantees, extending the range of applicability for generic Lipschitz losses. The core aim is the same as well known by [6]."
    },
    {
      "marker": "[10]",
      "context_window": "Approaches such as Random Fourier Features [21] have proven effective, while recent work onp-sparsifiedsketches[10]demonstratesthatstructuredsparsitycanofferbothcom-putational advantages and strong theoretical guarantees, extending the range of applicability for generic Lipschitz losses. The core aim is the same as well known by [6]."
    },
    {
      "marker": "[6]",
      "context_window": "Approaches such as Random Fourier Features [21] have proven effective, while recent work onp-sparsifiedsketches[10]demonstratesthatstructuredsparsitycanofferbothcom-putational advantages and strong theoretical guarantees, extending the range of applicability for generic Lipschitz losses. The core aim is the same as well known by [6]."
    },
    {
      "marker": "[37, 20]",
      "context_window": "Details regarding Sobolev spaces of vector-valued functions and their associated vvRKHS can be found in [37, 20]."
    },
    {
      "marker": "[28]",
      "context_window": "Figure 1: Illustration of the proposed network architecture (adapted from [28], Figure 1). The network consists of an input layer, one hidden layer, activation functionσ 1, final nonlinear transformationg, and an output layer."
    },
    {
      "marker": "[28]",
      "context_window": "Using a similar argument as in the proof of Theorem3, and applying inequality(10)from Theorem2in [28], yields the following results."
    },
    {
      "marker": "[15]",
      "context_window": "The Koopman-based bound is flexible and can be combined with other bounds, as demonstrated previously ([15], Section 4.4 and Proposition 19). This approach can be extended to our setting using the same technique. Specifically, the complexity of anL-layer, multi-output network can be decomposed into the Koopman-based bound for the firstllayers and the bound for the remaining L−llayers."
    },
    {
      "marker": "[30, 12]",
      "context_window": "For simplicity, our analysis considers single-output neural networks and RKHSs associated with the one-dimensional Brownian kernel as the func-tion spaces presented in Remark(2)-(i)in [28]. A further refinement involves combining the Koopman-based framework with established “peeling” techniques [30, 12]. This combination has the potential to yield a complexity bound of the form O(LYj=l+1∥Wj∥2,2lYj=1∥Wj∥)."
    },
    {
      "marker": "[21]",
      "context_window": "We adopt the framework of ([21], Sections2.1&3) to derive our excess risk bounds. Specifically, we assume that the true risk is minimized overHs0 at fHs0 := arg minf∈H s0 ERadmn[ℓ(f(X), Y)]. The existence of such a minimizer is a standard assumption [6, 10, 33, 39] and implies thatfHs0 has bounded norm ([10, 33], Remark2)."
    },
    {
      "marker": "[6, 10, 33, 39]",
      "context_window": "Specifically, we assume that the true risk is minimized overHs0 at fHs0 := arg minf∈H s0 ERadmn[ℓ(f(X), Y)]. The existence of such a minimizer is a standard assumption [6, 10, 33, 39] and implies thatfHs0 has bounded norm ([10, 33], Remark2)."
    },
    {
      "marker": "[10, 33]",
      "context_window": "The existence of such a minimizer is a standard assumption [6, 10, 33, 39] and implies thatfHs0 has bounded norm ([10, 33], Remark2). Following [10, 21], we also assume that the estimators obtained by Empirical Risk Minimization possess bounded norms."
    },
    {
      "marker": "[10, 21]",
      "context_window": "Following [10, 21], we also assume that the estimators obtained by Empirical Risk Minimization possess bounded norms. Letk 0/n=UDU ⊤ represent the eigendecomposition of the scaled Gram matrixk 0, whereD= diag(µ 1, . . . , µn)contains the eigenvalues ofk 0/nar-rangedindecreasingorder."
    },
    {
      "marker": "[10]",
      "context_window": "The statistical dimensiond n isdefinedin[10]astheminimalindexj∈ {1, . . . , n} such thatµj ≤δ 2n, withd n =nif no suchjexists. Definition 2.(k 0-satisfiability, [39])Letc >0be independent ofn,U 1 ∈ Rn×dn andU 2 ∈R n×(n−dn) be the left and right blocks of the matrixUpre-viously defined, andD 2 = diag(µdn+1, . . . , µn)."
    },
    {
      "marker": "[39]",
      "context_window": "The existence and uniqueness ofδ2n are well-established for any RKHS associated with a positive definite kernel, as demonstrated in [39]. Definition 2.(k 0-satisfiability, [39])Letc >0be independent ofn,U 1 ∈ Rn×dn andU 2 ∈R n×(n−dn) be the left and right blocks of the matrixUpre-viously defined, andD 2 = diag(µdn+1, . . . , µn)."
    },
    {
      "marker": "[10]",
      "context_window": "The following theorem’s proof directly applies Theorem4from [10], a key theoretical result underpinning this paper. [10]proposedp-sparsifiedsketches, demonstratingtheirk 0-satisfiability."
    },
    {
      "marker": "[10]",
      "context_window": "[10]proposedp-sparsifiedsketches, demonstratingtheirk 0-satisfiability. These sketches consist of i.i.d. Rademacher or centered Gaussian entries modulated by Bernoulli variables (parameterp, scaled for isometry). Sparsity, controlled byp, enables rewriting the sketch matrixSas a product of a sub-Gaussian and a reduced sub-sampling sketch, improving computational efficiency."
    },
    {
      "marker": "[10]",
      "context_window": "p-sparsified sketches arek0-satisfiable with high probability forc= 2√p 1 + p log (5) + 1 (see, [10], Theorem 5)."
    },
    {
      "marker": "[14]",
      "context_window": "Figure 2: Illustration of the proposed deep vvRKHS (adapted from [14], Figure 1). For the autoencoder, the encoder is represented byf1 ◦f 2, and the decoder byf 3 ◦f 4."
    },
    {
      "marker": "[14]",
      "context_window": "Inspired by recent advances in operator-theoretic approaches to deep learning, particularly the framework developed by [14], this section presents a new network architecture designed to capture intricate relationships in multi-task learn-ing. BystrategicallyemployingPFoperatorswithinadeepvvRKHSframework, we create a computationally tractable yet theoretically sound approach to deep kernel methods."
    },
    {
      "marker": "[41]",
      "context_window": "Then, forj= 1, . . . , L, we haveH j ≤ eHj, as shown in ([41], Proposition 17). Consequently, the Rademacher complexity can be bounded as bRm n (FL)≤ p κTr(M1) n sup (fj∈Fj)j ∥PfL−1 . . .Pf1 |eV(x)∥∥fL∥HL,(16) where we assumek1 satisfies Assumption 4."
    },
    {
      "marker": "[41]",
      "context_window": "In this case, a refinementG(L) withH L ⊂ H(L) (as function spaces, we con-sider vvRKHSsH (j) corresponding to the separable kernelsG(j) =k jA, where M≤AforA∈ B(R m)+ andj= 1, . . . , L) enlarges the candidate function space, diminishing approximation error. By appropriately refining the vvRKHS, we modulate the trade-off between approximation and sampling errors, mitigat-ing both overfitting and underfitting [41]."
    }
  ],
  "references_list": [
    {
      "ref_id": "[1]",
      "parsed": {
        "title": "Multi-task feature learning",
        "authors": [
          "Argyriou, A.",
          "Evgeniou, T.",
          "Pontil, M."
        ],
        "year": 2006,
        "venue": "Advances in Neural Information Processing Systems",
        "doi": null
      }
    },
    {
      "ref_id": "[2]",
      "parsed": {
        "title": "Spectrally-normalized margin bounds for neural networks",
        "authors": [
          "Bartlett, P.L.",
          "Foster, D.J.",
          "Telgarsky, M.J."
        ],
        "year": 2017,
        "venue": "Proceedings of Advances in Neural Information Processing Systems (NeurIPS)",
        "doi": null
      }
    },
    {
      "ref_id": "[3]",
      "parsed": {
        "title": "Benign overfitting in linearregression",
        "authors": [
          "Bartlett, P.L.",
          "Long, P.M.",
          "Lugosi, G.",
          "Tsigler, A."
        ],
        "year": 2020,
        "venue": "Proceedings of the National Academy of Sciences",
        "doi": null
      }
    },
    {
      "ref_id": "[4]",
      "parsed": {
        "title": "A kernel perspective for regularizing deep neural networks",
        "authors": [
          "Bietti, A.",
          "Mialon, G.",
          "Chen, D.",
          "Mairal, J."
        ],
        "year": 2019,
        "venue": "In Proceedings of the 36th International Conference on Machine Learning (ICML)",
        "doi": null
      }
    },
    {
      "ref_id": "[5]",
      "parsed": {
        "title": "A representer theorem for deep kernel learning",
        "authors": [
          "Bohn, B.",
          "Griebel, M.",
          "Rieger, C."
        ],
        "year": 2019,
        "venue": "Journal of Machine Learning Research",
        "doi": null
      }
    },
    {
      "ref_id": "[6]",
      "parsed": {
        "title": "Optimal rates for the regularized least-squares algorithm",
        "authors": [
          "Caponnetto, A.",
          "Vito, E.D."
        ],
        "year": 2007,
        "venue": "Foundations of Computational Mathematics",
        "doi": null
      }
    },
    {
      "ref_id": "[7]",
      "parsed": {
        "title": "Deep neural tangent kernel and laplace kernel have the same RKHS",
        "authors": [
          "Chen, L.",
          "Xu, S."
        ],
        "year": 2021,
        "venue": "In Proceedings of the 9th International Conference on Learning Representations (ICLR)",
        "doi": null
      }
    },
    {
      "ref_id": "[8]",
      "parsed": {
        "title": "Provable multi-task representation learning by two-layer relu neural networks",
        "authors": [
          "Collins, L.",
          "Hassani, H.",
          "Soltanolkotabi, M.",
          "Mokhtari, A.",
          "Shakkottai, S."
        ],
        "year": 2024,
        "venue": "Proceedings of the Forty-first International Conference on Machine Learning",
        "doi": null
      }
    },
    {
      "ref_id": "[9]",
      "parsed": {
        "title": "Multi-task learning with deep neural networks: A survey",
        "authors": [
          "Crawshaw, M."
        ],
        "year": 2020,
        "venue": "arXiv preprint arXiv:2009.09796",
        "doi": null
      }
    },
    {
      "ref_id": "[10]",
      "parsed": {
        "title": "Fast kernel methods for generic Lipschitz losses via p-sparsified sketches",
        "authors": [
          "El Ahmad, T.",
          "Laforgue, P.",
          "d’Alché Buc, F."
        ],
        "year": 2023,
        "venue": "Transactions on Machine Learning Research",
        "doi": null
      }
    },
    {
      "ref_id": "[11]",
      "parsed": {
        "title": "Multi-task deep learning as multi-objective optimization",
        "authors": [
          "Fatta, G.D.",
          "Nicosia, G.",
          "Ojha, V.",
          "Pardalos, P."
        ],
        "year": 2023,
        "venue": "Encyclopedia of Optimization",
        "doi": null
      }
    },
    {
      "ref_id": "[12]",
      "parsed": {
        "title": "Size-independent sample complexity of neural networks",
        "authors": [
          "Golowich, N.",
          "Rakhlin, A.",
          "Shamir, O."
        ],
        "year": 2018,
        "venue": "Proceedings of the 2018 Conference On Learning Theory (COLT)",
        "doi": null
      }
    },
    {
      "ref_id": "[13]",
      "parsed": {
        "title": "Size-independent sample complexity of neural networks",
        "authors": [
          "Golowich, N.",
          "Rakhlin, A.",
          "Shamir, O."
        ],
        "year": 2020,
        "venue": "Information and Inference: A Journal of the IMA",
        "doi": null
      }
    },
    {
      "ref_id": "[14]",
      "parsed": {
        "title": "Deep learning with kernels through rkhm and the perron-frobenius operator",
        "authors": [
          "Hashimoto, Y.",
          "Ikeda, M.",
          "Kadri, H."
        ],
        "year": 2023,
        "venue": "Advances in Neural Information Processing Systems",
        "doi": null
      }
    },
    {
      "ref_id": "[15]",
      "parsed": {
        "title": "Koopman-based generalization bound: New aspect for full-rank weights",
        "authors": [
          "Hashimoto, Y.",
          "Sonoda, S.",
          "Ishikawa, I.",
          "Nitanda, A.",
          "Suzuki, T."
        ],
        "year": 2024,
        "venue": "The Twelfth International Conference on Learning Representations",
        "doi": null
      }
    },
    {
      "ref_id": "[16]",
      "parsed": {
        "title": "Entangled kernels - beyond separability",
        "authors": [
          "Huusari, R.",
          "Kadri, H."
        ],
        "year": 2021,
        "venue": "Journal of Machine Learning Research",
        "doi": null
      }
    },
    {
      "ref_id": "[17]",
      "parsed": {
        "title": "Neural tangent kernel: Convergence and generalization in neural networks",
        "authors": [
          "Jacot, A.",
          "Gabriel, F.",
          "Hongler, C."
        ],
        "year": 2018,
        "venue": "In Proceedings of Advances in Neural Information Processing Systems (NeurIPS)",
        "doi": null
      }
    },
    {
      "ref_id": "[18]",
      "parsed": {
        "title": "Robust fine-tuning of deep neural networks with Hessian-based generalization guarantees",
        "authors": [
          "Ju, H.",
          "Li, D.",
          "Zhang, H.R."
        ],
        "year": 2022,
        "venue": "Proceedings of the 39th International Conference on Machine Learning (ICML)",
        "doi": null
      }
    },
    {
      "ref_id": "[19]",
      "parsed": {
        "title": "Autoencoding any data through kernel autoencoders",
        "authors": [
          "Laforgue, P.",
          "Clémençon, S.",
          "d’Alché Buc, F."
        ],
        "year": 2019,
        "venue": "Proceedings of the 22nd International Conference on Artificial Intelligence and Statistics (AISTATS)",
        "doi": null
      }
    },
    {
      "ref_id": "[20]",
      "parsed": {
        "title": "Towards optimal sobolev norm rates for the vector-valued regularized least-squares algorithm",
        "authors": [
          "Li, Z.",
          "Meunier, D.",
          "Mollenhauer, M.",
          "Gretton, A."
        ],
        "year": 2024,
        "venue": "Journal of Machine Learning Research",
        "doi": null
      }
    },
    {
      "ref_id": "[21]",
      "parsed": {
        "title": "Towards a unified analysis of random fourier features",
        "authors": [
          "Li, Z.",
          "Ton, J.F.",
          "Oglic, D.",
          "Sejdinovic, D."
        ],
        "year": 2021,
        "venue": "Journal of Machine Learning Research",
        "doi": null
      }
    },
    {
      "ref_id": "[22]",
      "parsed": {
        "title": "Implicit regularization of multi-task learning and finetuning in overparameterized neural networks",
        "authors": [
          "Lindsey, J.W.",
          "Lippl, S."
        ],
        "year": 2023,
        "venue": "arXiv preprint arXiv:2310.02396",
        "doi": null
      }
    },
    {
      "ref_id": "[23]",
      "parsed": {
        "title": "Randomized algorithms for matrices and data",
        "authors": [
          "Mahoney, M.W.",
          "et al."
        ],
        "year": 2011,
        "venue": "Foundations and Trends® in Machine Learning",
        "doi": null
      }
    },
    {
      "ref_id": "[24]",
      "parsed": {
        "title": "Convolutional kernel networks",
        "authors": [
          "Mairal, J.",
          "Koniusz, P.",
          "Harchaoui, Z.",
          "Schmid, C."
        ],
        "year": 2014,
        "venue": "In Proceedings of the Advances in Neural Information Processing Systems (NIPS)",
        "doi": null
      }
    },
    {
      "ref_id": "[25]",
      "parsed": {
        "title": "Benign, tempered, or catastrophic: Toward a refined taxonomy of overfitting",
        "authors": [
          "Mallinar, N.R.",
          "Simon, J.B.",
          "Abedsoltan, A.",
          "Pandit, P.",
          "Belkin, M.",
          "Nakkiran, P."
        ],
        "year": 2022,
        "venue": "In Proceedings of Advances in Neural Information Processing Systems (NeurIPS)",
        "doi": null
      }
    },
    {
      "ref_id": "[26]",
      "parsed": {
        "title": "Bounds for linear multi-task learning",
        "authors": [
          "Maurer, A."
        ],
        "year": 2006,
        "venue": "The Journal of Machine Learning Research",
        "doi": null
      }
    },
    {
      "ref_id": "[27]",
      "parsed": {
        "title": "On learning vector-valued functions",
        "authors": [
          "Micchelli, C.A.",
          "Pontil, M."
        ],
        "year": 2005,
        "venue": "Neural Computation",
        "doi": null
      }
    },
    {
      "ref_id": "[28]",
      "parsed": {
        "title": "On the koopman-based generalization bounds for multi-task deep learning",
        "authors": [
          "Mohammadigohari, M.",
          "Di Fatta, G.",
          "Nicosia, G.",
          "Pardalos, P."
        ],
        "year": 2025,
        "venue": "Proceedings of the International Conference on Learning and Discovery (LOD). Lecture Notes in Computer Science",
        "doi": null
      }
    },
    {
      "ref_id": "[29]",
      "parsed": {
        "title": "Foundations of Machine Learning",
        "authors": [
          "Mohri, M.",
          "Rostamizadeh, A.",
          "Talwalkar, A."
        ],
        "year": 2018,
        "venue": "MIT Press",
        "doi": null
      }
    },
    {
      "ref_id": "[30]",
      "parsed": {
        "title": "Norm-based capacity control in neural networks",
        "authors": [
          "Neyshabur, B.",
          "Tomioka, R.",
          "Srebro, N."
        ],
        "year": 2015,
        "venue": "Proceedings of the 2015 Conference on Learning Theory (COLT)",
        "doi": null
      }
    },
    {
      "ref_id": "[31]",
      "parsed": {
        "title": "The promises and pitfalls of deep kernel learning",
        "authors": [
          "Ober, S.W.",
          "Rasmussen, C.E.",
          "van der Wilk, M."
        ],
        "year": 2021,
        "venue": "Proceedings of the 37th Conference on Uncertainty in Artificial Intelligence (UAI)",
        "doi": null
      }
    },
    {
      "ref_id": "[32]",
      "parsed": {
        "title": "Excess risk bounds for multitask learning with trace norm regularization",
        "authors": [
          "Pontil, M.",
          "Maurer, A."
        ],
        "year": 2013,
        "venue": "Proceedings of the Conference on Learning Theory",
        "doi": null
      }
    },
    {
      "ref_id": "[33]",
      "parsed": {
        "title": "Generalization properties of learning with random features",
        "authors": [
          "Rudi, A.",
          "Rosasco, L."
        ],
        "year": 2017,
        "venue": "Advances in Neural Information Processing Systems (NeurIPS)",
        "doi": null
      }
    },
    {
      "ref_id": "[34]",
      "parsed": {
        "title": "Variation spaces for multi-output neural networks: Insights on multi-task learning and network compression",
        "authors": [
          "Shenouda, J.",
          "Parhi, R.",
          "Lee, K.",
          "Nowak, R.D."
        ],
        "year": 2024,
        "venue": "Journal of Machine Learning Research",
        "doi": null
      }
    },
    {
      "ref_id": "[35]",
      "parsed": {
        "title": "Scalable matrix-valued kernel learning for high-dimensional nonlinear multivariate regression and granger causality",
        "authors": [
          "Sindhwani, V.",
          "Minh, H.Q.",
          "Lozano, A.C."
        ],
        "year": 2013,
        "venue": "Proceedings of the 29th Conference on Uncertainty in Artificial Intelligence (UAI)",
        "doi": null
      }
    },
    {
      "ref_id": "[36]",
      "parsed": {
        "title": "Compression based bound for non-compressed network: unified generalization error analysis of large compressible deep neural network",
        "authors": [
          "Suzuki, T.",
          "Abe, H.",
          "Nishimura, T."
        ],
        "year": 2020,
        "venue": "Proceedings of the 8th International Conference on Learning Representations (ICLR)",
        "doi": null
      }
    },
    {
      "ref_id": "[37]",
      "parsed": {
        "title": "Approximation with matrix-valued kernels and highly effective error estimators for reduced basis approximations",
        "authors": [
          "Wittwar, D."
        ],
        "year": 2022,
        "venue": "Ph.D. thesis, Universität Stuttgart",
        "doi": null
      }
    },
    {
      "ref_id": "[38]",
      "parsed": {
        "title": "Sketching as a tool for numerical linear algebra",
        "authors": [
          "Woodruff, D.P."
        ],
        "year": 2014,
        "venue": "Foundations and Trends® in Theoretical Computer Science",
        "doi": null
      }
    },
    {
      "ref_id": "[39]",
      "parsed": {
        "title": "Randomized sketches for kernels: Fast and optimal nonparametric regression",
        "authors": [
          "Yang, Y.",
          "Pilanci, M.",
          "Wainwright, M.J.",
          "others if applicable"
        ],
        "year": 2017,
        "venue": "The Annals of Statistics",
        "doi": null
      }
    },
    {
      "ref_id": "[40]",
      "parsed": {
        "title": "Local rademacher complexity-based learning guarantees for multi-task learning",
        "authors": [
          "Yousefi, N.",
          "Lei, Y.",
          "Kloft, M.",
          "Mollaghasemi, M.",
          "Anagnostopoulos, G.C."
        ],
        "year": 2018,
        "venue": "The Journal of Machine Learning Research",
        "doi": null
      }
    },
    {
      "ref_id": "[41]",
      "parsed": {
        "title": "Refinement of operator-valued reproducing kernels",
        "authors": [
          "Zhang, H.",
          "Xu, Y.",
          "Zhang, Q."
        ],
        "year": 2012,
        "venue": "Journal of Machine Learning Research",
        "doi": null
      }
    }
  ]
}