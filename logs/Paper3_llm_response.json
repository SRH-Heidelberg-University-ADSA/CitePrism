{
  "metadata": {
    "title": "Operator-Based Generalization Bound for Deep Learning: Insights on Multi-Task Learning",
    "authors": [
      "Mahdi Mohammadigohari",
      "Giuseppe Di Fatta",
      "Giuseppe Nicosia",
      "Panos M. Pardalos"
    ],
    "abstract": "This paper presents novel generalization bounds for vector-valued neural networks and deep kernel methods, focusing on multi-task learning through an operator-theoretic framework. Our key development lies in strategically combining a Koopman-based approach with existing techniques, achieving tighter generalization guarantees compared to traditional norm-based bounds. To mitigate computational challenges associated with Koopman-based methods, we introduce sketching techniques applicable to vector-valued neural networks. These techniques yield excess risk bounds under generic Lipschitz losses, providing performance guarantees for applications including robust and multiple quantile regression. Furthermore, we propose a novel deep learning framework—deep vector-valued reproducing kernel Hilbert spaces (vvRKHS)—leveraging Perron-Frobenius (PF) operators to enhance deep kernel methods. We derive a new Rademacher generalization bound for this framework, explicitly addressing underfitting and overfitting through kernel refinement strategies. This work offers novel insights into the generalization properties of multi-task learning with deep learning architectures, an area that has been relatively unexplored until recent developments."
  },
  "citations_in_text": [
    {
      "marker": "[9, 11]",
      "context_window": "Despite the remarkable empirical successes of deep learning across diverse fields [9, 11], a significant gap remains in our theoretical understanding of their generalization capabilities, especially in multi-task learning scenarios. This work is specifically motivated by the need to overcome the limitations of existing generalization bounds in capturing the performance of deep learning models and deep kernel methods in multi-task settings, an area where a deeper understanding of network structure, operator theory, and adaptive kernel refinement strategies is essential for realizing robust and reliable performance."
    },
    {
      "marker": "[31]",
      "context_window": "Traditionally distinct, kernel methods and deep neural networks are now being actively investigated for their interconnections. A key perspective is deep kernel learning [31], where composite functions in reproducing kernel Hilbert spaces (RKHSs) are learned from data, and representer theorems ensure data-dependent solutions [5, 19]. This blends deep neural network flexibility with the theoretical rigor of kernel methods."
    },
    {
      "marker": "[5, 19]",
      "context_window": "A key perspective is deep kernel learning [31], where composite functions in reproducing kernel Hilbert spaces (RKHSs) are learned from data, and representer theorems ensure data-dependent solutions [5, 19]. This blends deep neural network flexibility with the theoretical rigor of kernel methods. Relatedly, the neural tangent kernel [7, 17] and convolutional kernel networks [24] enable kernel-based analysis of neural networks, and kernel-inspired regularization has been explored for deep learning [4]."
    },
    {
      "marker": "[7, 17]",
      "context_window": "This blends deep neural network flexibility with the theoretical rigor of kernel methods. Relatedly, the neural tangent kernel [7, 17] and convolutional kernel networks [24] enable kernel-based analysis of neural networks, and kernel-inspired regularization has been explored for deep learning [4]. The generalization properties of both kernel methods and deep neural networks have been extensively investigated, with Rademacher complexity [29] serving as a central tool for bounding generalization errors."
    },
    {
      "marker": "[24]",
      "context_window": "This blends deep neural network flexibility with the theoretical rigor of kernel methods. Relatedly, the neural tangent kernel [7, 17] and convolutional kernel networks [24] enable kernel-based analysis of neural networks, and kernel-inspired regularization has been explored for deep learning [4]. The generalization properties of both kernel methods and deep neural networks have been extensively investigated, with Rademacher complexity [29] serving as a central tool for bounding generalization errors."
    },
    {
      "marker": "[4]",
      "context_window": "Relatedly, the neural tangent kernel [7, 17] and convolutional kernel networks [24] enable kernel-based analysis of neural networks, and kernel-inspired regularization has been explored for deep learning [4]. The generalization properties of both kernel methods and deep neural networks have been extensively investigated, with Rademacher complexity [29] serving as a central tool for bounding generalization errors. For kernel methods, Rademacher complexity bounds can be derived directly through the reproducing property."
    },
    {
      "marker": "[29]",
      "context_window": "Relatedly, the neural tangent kernel [7, 17] and convolutional kernel networks [24] enable kernel-based analysis of neural networks, and kernel-inspired regularization has been explored for deep learning [4]. The generalization properties of both kernel methods and deep neural networks have been extensively investigated, with Rademacher complexity [29] serving as a central tool for bounding generalization errors. For kernel methods, Rademacher complexity bounds can be derived directly through the reproducing property."
    },
    {
      "marker": "[16, 19, 35]",
      "context_window": "For kernel methods, Rademacher complexity bounds can be derived directly through the reproducing property. Extensions of this approach have produced generalization bounds for deep kernel methods and vector-valued reproducing kernel Hilbert spaces (vvRKHSs) [16, 19, 35]. Complementing this, a significant body of research has focused on generalization bounds specific to deep neural networks [2, 13, 14, 15, 18, 30, 36]."
    },
    {
      "marker": "[2, 13, 14, 15, 18, 30, 36]",
      "context_window": "Extensions of this approach have produced generalization bounds for deep kernel methods and vector-valued reproducing kernel Hilbert spaces (vvRKHSs) [16, 19, 35]. Complementing this, a significant body of research has focused on generalization bounds specific to deep neural networks [2, 13, 14, 15, 18, 30, 36]. A recent line of research [3, 25] has focused on analyzing generalization performance through the lens of benign overfitting."
    },
    {
      "marker": "[3, 25]",
      "context_window": "Complementing this, a significant body of research has focused on generalization bounds specific to deep neural networks [2, 13, 14, 15, 18, 30, 36]. A recent line of research [3, 25] has focused on analyzing generalization performance through the lens of benign overfitting. Despite these advances, a dedicated generalization analysis method for deep vector-valued kernel learning and multi-output deep neural networks remains underdeveloped, hindering the derivation of tighter, more meaningful generalization error bounds."
    },
    {
      "marker": "[28]",
      "context_window": "This paper introduces novel generalization bounds for vector-valued NNs in the multi-task learning setting. In [28], we initiated an operator-theoretic approach for analyzing generalization in multi-task deep learning, focusing on full-rank weights and leveraging Koopman operators. We derived a tighter bound compared to the existing Koopman-based bound [15] by applying a new function space."
    },
    {
      "marker": "[15]",
      "context_window": "In [28], we initiated an operator-theoretic approach for analyzing generalization in multi-task deep learning, focusing on full-rank weights and leveraging Koopman operators. We derived a tighter bound compared to the existing Koopman-based bound [15] by applying a new function space. In this work, the first part continues this line by introducing input space sketching—a dimensionality reduction technique to enhance practical applicability in large-scale settings."
    },
    {
      "marker": "[14]",
      "context_window": "In this work, the first part continues this line by introducing input space sketching—a dimensionality reduction technique to enhance practical applicability in large-scale settings. Drawing inspiration from Kernel Autoencoders (KAEs) and leveraging the framework of reproducing kernel HilbertC∗-modules—aC ∗-algebra-based extension of RKHS theory—[14] pioneered deep RKHM, a deep architecture built by composing functions within RKHMs using the Perron-Frobenius (PF) operator. We strategically combine this approach with existing techniques and, in the second part, we presents a new network architecture, deep vvRKHS, constructed through using PF operators."
    },
    {
      "marker": "[28, 15]",
      "context_window": "We strategically combine this approach with existing techniques and, in the second part, we presents a new network architecture, deep vvRKHS, constructed through using PF operators. This architecture loosens assumptions on weight matrices and activation functions from previous models in [28, 15] by refining the operator product representation of network layers, resulting in a more compact and expressive structure. By taking these points into consideration, to enhance generalization performance via our approach, we make the following contributions:"
    },
    {
      "marker": "[1]",
      "context_window": "Multi-TaskLearning:Theadvantagesofmulti-tasklearninghavebeenextensivelyexplored inthemachinelearningliterature[1]. Recenttheoreticalresearch has focused on properties of multi-task neural networks and how shared representations and task relationships influence generalization [22, 8, 34]. Initial generalization results in multi-task learning include Rademacher complexity-based bounds for linear classification [26], subsequently refined to derive risk estimates in trace norm regularized models [32]."
    },
    {
      "marker": "[22, 8, 34]",
      "context_window": "Multi-TaskLearning:Theadvantagesofmulti-tasklearninghavebeenextensivelyexplored inthemachinelearningliterature[1]. Recenttheoreticalresearch has focused on properties of multi-task neural networks and how shared representations and task relationships influence generalization [22, 8, 34]. Initial generalization results in multi-task learning include Rademacher complexity-based bounds for linear classification [26], subsequently refined to derive risk estimates in trace norm regularized models [32]."
    },
    {
      "marker": "[26]",
      "context_window": "Recenttheoreticalresearch has focused on properties of multi-task neural networks and how shared representations and task relationships influence generalization [22, 8, 34]. Initial generalization results in multi-task learning include Rademacher complexity-based bounds for linear classification [26], subsequently refined to derive risk estimates in trace norm regularized models [32]. More recently, sharper risk bounds leveraging local Rademacher complexity analysed the role of common regularization strategies [40]."
    },
    {
      "marker": "[32]",
      "context_window": "Initial generalization results in multi-task learning include Rademacher complexity-based bounds for linear classification [26], subsequently refined to derive risk estimates in trace norm regularized models [32]. More recently, sharper risk bounds leveraging local Rademacher complexity analysed the role of common regularization strategies [40]. To address the current scarcity of multi-task deep learning theories, this work introduces a novel framework to analyzing the generalization characteristics of functions learned by vector-valued deep learning architectures with use of transfer operators, providing new insights into multi-task learning with neural networks."
    },
    {
      "marker": "[40]",
      "context_window": "subsequently refined to derive risk estimates in trace norm regularized models [32]. More recently, sharper risk bounds leveraging local Rademacher complexity analysed the role of common regularization strategies [40]. To address the current scarcity of multi-task deep learning theories, this work introduces a novel framework to analyzing the generalization characteristics of functions learned by vector-valued deep learning architectures with use of transfer operators, providing new insights into multi-task learning with neural networks. Sketching:To alleviate the computational demands of kernel methods, sketching techniques have emerged as a powerful tool for approximating kernel matrices and reducing memory requirements [23, 38, 39]."
    },
    {
      "marker": "[23, 38, 39]",
      "context_window": "More recently, sharper risk bounds leveraging local Rademacher complexity analysed the role of common regularization strategies [40]. To address the current scarcity of multi-task deep learning theories, this work introduces a novel framework to analyzing the generalization characteristics of functions learned by vector-valued deep learning architectures with use of transfer operators, providing new insights into multi-task learning with neural networks. Sketching:To alleviate the computational demands of kernel methods, sketching techniques have emerged as a powerful tool for approximating kernel matrices and reducing memory requirements [23, 38, 39]. By employing randomized linear projections, sketching can enable near-optimal nonparametric regression [39] and provide efficient solutions for large-scale problems."
    },
    {
      "marker": "[39]",
      "context_window": "Sketching:To alleviate the computational demands of kernel methods, sketching techniques have emerged as a powerful tool for approximating kernel matrices and reducing memory requirements [23, 38, 39]. By employing randomized linear projections, sketching can enable near-optimal nonparametric regression [39] and provide efficient solutions for large-scale problems. Approaches such as Random Fourier Features [21] have proven effective, while recent work onp-sparsifiedsketches[10]demonstratesthatstructuredsparsitycanofferbothcomputational advantages and strong theoretical guarantees, extending the range of applicability for generic Lipschitz losses."
    },
    {
      "marker": "[21]",
      "context_window": "By employing randomized linear projections, sketching can enable near-optimal nonparametric regression [39] and provide efficient solutions for large-scale problems. Approaches such as Random Fourier Features [21] have proven effective, while recent work onp-sparsifiedsketches[10]demonstratesthatstructuredsparsitycanofferbothcomputational advantages and strong theoretical guarantees, extending the range of applicability for generic Lipschitz losses. The core aim is the same as well known by [6]."
    },
    {
      "marker": "[10]",
      "context_window": "Approaches such as Random Fourier Features [21] have proven effective, while recent work onp-sparsifiedsketches[10]demonstratesthatstructuredsparsitycanofferbothcomputational advantages and strong theoretical guarantees, extending the range of applicability for generic Lipschitz losses. The core aim is the same as well known by [6]. This section presents our notation (Section 2.1) and the required mathematical background (Section 2.2)."
    },
    {
      "marker": "[6]",
      "context_window": "while recent work onp-sparsifiedsketches[10]demonstratesthatstructuredsparsitycanofferbothcomputational advantages and strong theoretical guarantees, extending the range of applicability for generic Lipschitz losses. The core aim is the same as well known by [6]. This section presents our notation (Section 2.1) and the required mathematical background (Section 2.2)."
    },
    {
      "marker": "[37, 20]",
      "context_window": "Fors > d/2,Hs(Rd,Rm)is the Sobolev spaceW s,2(Rd,Rm). Details regarding Sobolev spaces of vector-valued functions and their associated vvRKHS can be found in [37, 20]. We study the general setting of multi-output (vector-valued) supervised learning."
    },
    {
      "marker": "[6, 33, 21]",
      "context_window": "Specifically, Assumption 1, the attainability condition, is a common starting point in kernel literature [6, 33, 21], positing the existence of a risk minimizer within our hypothesis space. Assumption 2 addresses complexity control by restricting the hypothesis space to a unit ball, allowing us to normalize without loss of generality. Assumption 3, the Lipschitz continuity of the loss function, is a classical requirement for bounding generalization error."
    },
    {
      "marker": "[28]",
      "context_window": "Finally, Assumption 5, pertaining to the properties of the final nonlinear transformation, is justified by Remark 1 and Lemma 1 of [28]. These assumptions, while potentially restrictive, enable us to establish meaningful theoretical guarantees. We proceed by bounding the Rademacher complexity, analyzing the case where weight matrices are invertible or injective."
    },
    {
      "marker": "[28]",
      "context_window": "Using a similar argument as in the proof of Theorem3, and applying inequality(10)from Theorem2in [28], yields the following results. Lemma 1.The Rademacher complexity bRm n (Finj)is bounded as bRm n (Finj)≤∥g∥ HsL(RdL,Rm) rκ n Tr(M) sup Wl∈W C,D l LY l=1 Gl ·sup ω∈ran(Wl) 1 +∥W⊤ l ω∥2 2 1 +∥ω∥2 2 sl−1/2 1 det(W⊤ l Wl)1/4 L−1Y l=1 ∥Kσl∥.(5)"
    },
    {
      "marker": "[15]",
      "context_window": "The Koopman-based bound is flexible and can be combined with other bounds, as demonstrated previously ([15], Section 4.4 and Proposition 19). This approach can be extended to our setting using the same technique. Specifically, the complexity of anL-layer, multi-output network can be decomposed into the Koopman-based bound for the firstllayers and the bound for the remaining L−llayers."
    },
    {
      "marker": "[28]",
      "context_window": "Remark 1.For simplicity, our analysis considers single-output neural networks and RKHSs associated with the one-dimensional Brownian kernel as the function spaces presented in Remark(2)-(i)in [28]. A further refinement involves combining the Koopman-based framework with established “peeling” techniques [30, 12]. This combination has the potential to yield a complexity bound of the form O   LY j=l+1 ∥Wj∥2,2 lY j=1 ∥Wj∥  ."
    },
    {
      "marker": "[30, 12]",
      "context_window": "Remark 1.For simplicity, our analysis considers single-output neural networks and RKHSs associated with the one-dimensional Brownian kernel as the function spaces presented in Remark(2)-(i)in [28]. A further refinement involves combining the Koopman-based framework with established “peeling” techniques [30, 12]. This combination has the potential to yield a complexity bound of the form O   LY j=l+1 ∥Wj∥2,2 lY j=1 ∥Wj∥  ."
    },
    {
      "marker": "[27]",
      "context_window": "whereℓ:R m ×Rm →Rrepresents a loss function satisfying the following conditions: for ally∈R m, the mappingz7→ℓ(z,y)is proper, lower semi-continuous, and convex. According to the vector-valued representer theorem [27], the minimizer of Problem (11) admits the representationˆfn = Pn j=1 Ks0 (·,x j) ˆαj=Pn j=1 ks0 (·,x j)Mˆαj, where the coefficient matrixˆA= ( ˆα1, . . . ,ˆαn)⊤ ∈R n×m solves the optimization problem min A∈Rn×m 1 n Pn i=1 ℓ([k0AM]⊤ i: ,y i) +λn 2 Tr(k0AMA⊤)."
    },
    {
      "marker": "[21]",
      "context_window": "In this framework, sketching entails replacing the matrixAwith a sketched approximationS ⊤Γ, whereS∈R s×n denotes a sketch matrix andΓ∈R s×m parameterizes the reduced-dimensional representation. Consequently, the solution to the sketched problem is given by˜fs = Pn j=1 k(·,x j)M[S⊤ ˜Γ]j:, with ˜Γ∈R s×m obtained by minimizing 1 n Pn i=1 ℓ([k0S⊤ ˜ΓM]i:,y i) +λn 2 Tr(Sk0S⊤ ˜ΓM˜Γ⊤). We adopt the framework of ([21], Sections2.1&3) to derive our excess risk bounds."
    },
    {
      "marker": "[6, 10, 33, 39]",
      "context_window": "We adopt the framework of ([21], Sections2.1&3) to derive our excess risk bounds. Specifically, we assume that the true risk is minimized overHs0 at fHs0 := arg minf∈H s0 ERadmn[ℓ(f(X), Y)]. The existence of such a minimizer is a standard assumption [6, 10, 33, 39] and implies thatfHs0 has bounded norm ([10, 33], Remark2)."
    },
    {
      "marker": "[10, 33]",
      "context_window": "The existence of such a minimizer is a standard assumption [6, 10, 33, 39] and implies thatfHs0 has bounded norm ([10, 33], Remark2). Following [10, 21], we also assume that the estimators obtained by Empirical Risk Minimization possess bounded norms. Letk 0/n=UDU ⊤ represent the eigendecomposition of the scaled Gram matrixk 0, whereD= diag(µ 1, . . . , µn)contains the eigenvalues ofk 0/nar rangedindecreasingorder."
    },
    {
      "marker": "[10, 21]",
      "context_window": "implies thatfHs0 has bounded norm ([10, 33], Remark2). Following [10, 21], we also assume that the estimators obtained by Empirical Risk Minimization possess bounded norms. Letk 0/n=UDU ⊤ represent the eigendecomposition of the scaled Gram matrixk 0, whereD= diag(µ 1, . . . , µn)contains the eigenvalues ofk 0/nar rangedindecreasingorder."
    },
    {
      "marker": "[39]",
      "context_window": "Defineδ 2 n asthecriticalradiusofk 0/n, characterized as the minimal value for which the functionψ(δn) = 1 n Pn i=1 min(δ2 n, µi) 1/2 is bounded above byδ2 n. The existence and uniqueness ofδ2 n are well-established for any RKHS associated with a positive definite kernel, as demonstrated in [39]. Thestatisticaldimensiond n isdefinedin[10]astheminimalindexj∈ {1, . . . , n} such thatµj ≤δ 2 n, withd n =nif no suchjexists."
    },
    {
      "marker": "[10]",
      "context_window": "The existence and uniqueness ofδ2 n are well-established for any RKHS associated with a positive definite kernel, as demonstrated in [39]. Thestatisticaldimensiond n isdefinedin[10]astheminimalindexj∈ {1, . . . , n} such thatµj ≤δ 2 n, withd n =nif no suchjexists. Definition 2.(k 0-satisfiability, [39])Letc >0be independent ofn,U 1 ∈ Rn×dn andU 2 ∈R n×(n−dn) be the left and right blocks of the matrixUpre viously defined, andD 2 = diag(µdn+1, . . . , µn)."
    },
    {
      "marker": "[39]",
      "context_window": "Thestatisticaldimensiond n isdefinedin[10]astheminimalindexj∈ {1, . . . , n} such thatµj ≤δ 2 n, withd n =nif no suchjexists. Definition 2.(k 0-satisfiability, [39])Letc >0be independent ofn,U 1 ∈ Rn×dn andU 2 ∈R n×(n−dn) be the left and right blocks of the matrixUpre viously defined, andD 2 = diag(µdn+1, . . . , µn). A matrixSis said to bek 0- satisfiableforcif we have ∥(SU1)⊤SU1 −I dn∥ ≤1 2 ,and∥SU 2D1/2 2 ∥ ≤cδ n. (12)"
    },
    {
      "marker": "[10]",
      "context_window": "The following theorem’s proof directly applies Theorem4from [10], a key theoretical result underpinning this paper. Corollary 1.Suppose that Assumptions 1 to 6 hold, thatK s0 =k s0 Mis a decomposable kernel withMinvertible, and letC= 1+ √ 6c, with c the constant from Assumption 6. Then for anyδ∈(0,1), with probability at least1−δ, we have R ˜fs ≤ R(fHs0 ) +JℓC p λn +∥M∥δ 2n + λn 2 + 8L r κTr(M) n + 2 r 8 log(4/δ) n . (13) 11 [10]proposedp-sparsifiedsketches, demonstratingtheirk 0-satisfiability."
    },
    {
      "marker": "[10]",
      "context_window": "Then for anyδ∈(0,1), with probability at least1−δ, we have R ˜fs ≤ R(fHs0 ) +JℓC p λn +∥M∥δ 2n + λn 2 + 8L r κTr(M) n + 2 r 8 log(4/δ) n . (13) 11 [10]proposedp-sparsifiedsketches, demonstratingtheirk 0-satisfiability. These sketches consist of i.i.d. Rademacher or centered Gaussian entries modulated by Bernoulli variables (parameterp, scaled for isometry)."
    },
    {
      "marker": "[14]",
      "context_window": "Inspired by recent advances in operator-theoretic approaches to deep learning, particularly the framework developed by [14], this section presents a new network architecture designed to capture intricate relationships in multi-task learning. BystrategicallyemployingPFoperatorswithinadeepvvRKHSframework, we create a computationally tractable yet theoretically sound approach to deep kernel methods."
    },
    {
      "marker": "[41]",
      "context_window": "DefineeHj as the vvRKHS corresponding to the separable kernelK j =k jM. Then, forj= 1, . . . , L, we haveH j ≤ eHj, as shown in ([41], Proposition 17). Consequently, the Rademacher complexity can be bounded as bRm n (FL)≤ p κTr(M1) n sup (fj∈Fj)j ∥PfL−1 . . .Pf1 |eV(x)∥∥fL∥HL,(16) where we assumek1 satisfies Assumption 4."
    },
    {
      "marker": "[41]",
      "context_window": "By appropriately refining the vvRKHS, we modulate the trade-off between approximation and sampling errors, mitigating both overfitting and underfitting [41]."
    }
  ],
  "references_list": [
    {
      "ref_id": "[1]",
      "parsed": {
        "title": "Multi-task feature learning",
        "authors": [
          "A. Argyriou",
          "T. Evgeniou",
          "M. Pontil"
        ],
        "year": 2006,
        "venue": "Advances in Neural Information Processing Systems",
        "doi": null
      }
    },
    {
      "ref_id": "[2]",
      "parsed": {
        "title": "Spectrally-normalized margin bounds for neural networks",
        "authors": [
          "P.L. Bartlett",
          "D.J. Foster",
          "M.J. Telgarsky"
        ],
        "year": 2017,
        "venue": "Proceedings of Advances in Neural Information Processing Systems (NeurIPS)",
        "doi": null
      }
    },
    {
      "ref_id": "[3]",
      "parsed": {
        "title": "Benign overfitting in linear regression",
        "authors": [
          "P.L. Bartlett",
          "P.M. Long",
          "G. Lugosi",
          "A. Tsigler"
        ],
        "year": 2020,
        "venue": "Proceedings of the National Academy of Sciences",
        "doi": null
      }
    },
    {
      "ref_id": "[4]",
      "parsed": {
        "title": "A kernel perspective for regularizing deep neural networks",
        "authors": [
          "A. Bietti",
          "G. Mialon",
          "D. Chen",
          "J. Mairal"
        ],
        "year": 2019,
        "venue": "Proceedings of the 36th International Conference on Machine Learning (ICML)",
        "doi": null
      }
    },
    {
      "ref_id": "[5]",
      "parsed": {
        "title": "A representer theorem for deep kernel learning",
        "authors": [
          "B. Bohn",
          "M. Griebel",
          "C. Rieger"
        ],
        "year": 2019,
        "venue": "Journal of Machine Learning Research",
        "doi": null
      }
    },
    {
      "ref_id": "[6]",
      "parsed": {
        "title": "Optimal rates for the regularized least-squares algorithm",
        "authors": [
          "A. Caponnetto",
          "E.D. Vito"
        ],
        "year": 2007,
        "venue": "Foundations of Computational Mathematics",
        "doi": null
      }
    },
    {
      "ref_id": "[7]",
      "parsed": {
        "title": "Deep neural tangent kernel and laplace kernel have the same RKHS",
        "authors": [
          "L. Chen",
          "S. Xu"
        ],
        "year": 2021,
        "venue": "Proceedings of the 9th International Conference on Learning Representations (ICLR)",
        "doi": null
      }
    },
    {
      "ref_id": "[8]",
      "parsed": {
        "title": "Provable multi-task representation learning by two-layer relu neural networks",
        "authors": [
          "L. Collins",
          "H. Hassani",
          "M. Soltanolkotabi",
          "A. Mokhtari",
          "S. Shakkottai"
        ],
        "year": 2024,
        "venue": "Proceedings of the Forty-first International Conference on Machine Learning",
        "doi": null
      }
    },
    {
      "ref_id": "[9]",
      "parsed": {
        "title": "Multi-task learning with deep neural networks: A survey",
        "authors": [
          "M. Crawshaw"
        ],
        "year": 2020,
        "venue": "arXiv preprint arXiv:2009.09796",
        "doi": null
      }
    },
    {
      "ref_id": "[10]",
      "parsed": {
        "title": "Fast kernel methods for generic Lipschitz losses via p-sparsified sketches",
        "authors": [
          "T. El Ahmad",
          "P. Laforgue",
          "F. d’Alché Buc"
        ],
        "year": 2023,
        "venue": "Transactions on Machine Learning Research",
        "doi": null
      }
    },
    {
      "ref_id": "[11]",
      "parsed": {
        "title": "Multi-task deep learning as multi-objective optimization",
        "authors": [
          "G.D. Fatta",
          "G. Nicosia",
          "V. Ojha",
          "P. Pardalos"
        ],
        "year": 2023,
        "venue": "Encyclopedia of Optimization, Springer",
        "doi": null
      }
    },
    {
      "ref_id": "[12]",
      "parsed": {
        "title": "Size-independent sample complexity of neural networks",
        "authors": [
          "N. Golowich",
          "A. Rakhlin",
          "O. Shamir"
        ],
        "year": 2018,
        "venue": "Proceedings of the 2018 Conference On Learning Theory (COLT)",
        "doi": null
      }
    },
    {
      "ref_id": "[13]",
      "parsed": {
        "title": "Size-independent sample complexity of neural networks",
        "authors": [
          "N. Golowich",
          "A. Rakhlin",
          "O. Shamir"
        ],
        "year": 2020,
        "venue": "Information and Inference: A Journal of the IMA",
        "doi": null
      }
    },
    {
      "ref_id": "[14]",
      "parsed": {
        "title": "Deep learning with kernels through rkhm and the perron-frobenius operator",
        "authors": [
          "Y. Hashimoto",
          "M. Ikeda",
          "H. Kadri"
        ],
        "year": 2023,
        "venue": "Advances in Neural Information Processing Systems",
        "doi": null
      }
    },
    {
      "ref_id": "[15]",
      "parsed": {
        "title": "Koopman-based generalization bound: New aspect for full-rank weights",
        "authors": [
          "Y. Hashimoto",
          "S. Sonoda",
          "I. Ishikawa",
          "A. Nitanda",
          "T. Suzuki"
        ],
        "year": 2024,
        "venue": "The Twelfth International Conference on Learning Representations",
        "doi": null
      }
    },
    {
      "ref_id": "[16]",
      "parsed": {
        "title": "Entangled kernels - beyond separability",
        "authors": [
          "R. Huusari",
          "H. Kadri"
        ],
        "year": 2021,
        "venue": "Journal of Machine Learning Research",
        "doi": null
      }
    },
    {
      "ref_id": "[17]",
      "parsed": {
        "title": "Neural tangent kernel: Convergence and generalization in neural networks",
        "authors": [
          "A. Jacot",
          "F. Gabriel",
          "C. Hongler"
        ],
        "year": 2018,
        "venue": "Proceedings of Advances in Neural Information Processing Systems (NeurIPS)",
        "doi": null
      }
    },
    {
      "ref_id": "[18]",
      "parsed": {
        "title": "Robust fine-tuning of deep neural networks with Hessian-based generalization guarantees",
        "authors": [
          "H. Ju",
          "D. Li",
          "H.R. Zhang"
        ],
        "year": 2022,
        "venue": "Proceedings of the 39th International Conference on Machine Learning (ICML)",
        "doi": null
      }
    },
    {
      "ref_id": "[19]",
      "parsed": {
        "title": "Autoencoding any data through kernel autoencoders",
        "authors": [
          "P. Laforgue",
          "S. Clémençon",
          "F. d’Alché Buc"
        ],
        "year": 2019,
        "venue": "Proceedings of the 22nd International Conference on Artificial Intelligence and Statistics (AISTATS)",
        "doi": null
      }
    },
    {
      "ref_id": "[20]",
      "parsed": {
        "title": "Towards optimal sobolev norm rates for the vector-valued regularized least-squares algorithm",
        "authors": [
          "Z. Li",
          "D. Meunier",
          "M. Mollenhauer",
          "A. Gretton"
        ],
        "year": 2024,
        "venue": "Journal of Machine Learning Research",
        "doi": null
      }
    },
    {
      "ref_id": "[21]",
      "parsed": {
        "title": "Towards a unified analysis of random fourier features",
        "authors": [
          "Z. Li",
          "J.F. Ton",
          "D. Oglic",
          "D. Sejdinovic"
        ],
        "year": 2021,
        "venue": "Journal of Machine Learning Research",
        "doi": null
      }
    },
    {
      "ref_id": "[22]",
      "parsed": {
        "title": "Implicit regularization of multi-task learning and finetuning in overparameterized neural networks",
        "authors": [
          "J.W. Lindsey",
          "S. Lippl"
        ],
        "year": 2023,
        "venue": "arXiv preprint arXiv:2310.02396",
        "doi": null
      }
    },
    {
      "ref_id": "[23]",
      "parsed": {
        "title": "Randomized algorithms for matrices and data",
        "authors": [
          "M.W. Mahoney",
          "et al."
        ],
        "year": 2011,
        "venue": "Foundations and Trends®in Machine Learning",
        "doi": null
      }
    },
    {
      "ref_id": "[24]",
      "parsed": {
        "title": "Convolutional kernel networks",
        "authors": [
          "J. Mairal",
          "P. Koniusz",
          "Z. Harchaoui",
          "C. Schmid"
        ],
        "year": 2014,
        "venue": "Proceedings of the Advances in Neural Information Processing Systems (NIPS)",
        "doi": null
      }
    },
    {
      "ref_id": "[25]",
      "parsed": {
        "title": "Benign, tempered, or catastrophic: Toward a refined taxonomy of overfitting",
        "authors": [
          "N.R. Mallinar",
          "J.B. Simon",
          "A. Abedsoltan",
          "P. Pandit",
          "M. Belkin",
          "P. Nakkiran"
        ],
        "year": 2022,
        "venue": "Proceedings of Advances in Neural Information Processing Systems (NeurIPS)",
        "doi": null
      }
    },
    {
      "ref_id": "[26]",
      "parsed": {
        "title": "Bounds for linear multi-task learning",
        "authors": [
          "A. Maurer"
        ],
        "year": 2006,
        "venue": "The Journal of Machine Learning Research",
        "doi": null
      }
    },
    {
      "ref_id": "[27]",
      "parsed": {
        "title": "On learning vector-valued functions",
        "authors": [
          "C.A. Micchelli",
          "M. Pontil"
        ],
        "year": 2005,
        "venue": "Neural Computation",
        "doi": null
      }
    },
    {
      "ref_id": "[28]",
      "parsed": {
        "title": "On the koopman-based generalization bounds for multi-task deep learning",
        "authors": [
          "M. Mohammadigohari",
          "G. Di Fatta",
          "G. Nicosia",
          "P. Pardalos"
        ],
        "year": 2025,
        "venue": "Proceedings of the International Conference on Learning and Discovery (LOD), Lecture Notes in Computer Science, Springer, Cham",
        "doi": null
      }
    },
    {
      "ref_id": "[29]",
      "parsed": {
        "title": "Foundations of Machine Learning",
        "authors": [
          "M. Mohri",
          "A. Rostamizadeh",
          "A. Talwalkar"
        ],
        "year": 2018,
        "venue": "MIT Press, Cambridge, MA",
        "doi": null
      }
    },
    {
      "ref_id": "[30]",
      "parsed": {
        "title": "Norm-based capacity control in neural networks",
        "authors": [
          "B. Neyshabur",
          "R. Tomioka",
          "N. Srebro"
        ],
        "year": 2015,
        "venue": "Proceedings of the 2015 Conference on Learning Theory (COLT)",
        "doi": null
      }
    },
    {
      "ref_id": "[31]",
      "parsed": {
        "title": "The promises and pitfalls of deep kernel learning",
        "authors": [
          "S.W. Ober",
          "C.E. Rasmussen",
          "M. van der Wilk"
        ],
        "year": 2021,
        "venue": "Proceedings of the 37th Conference on Uncertainty in Artificial Intelligence (UAI)",
        "doi": null
      }
    },
    {
      "ref_id": "[32]",
      "parsed": {
        "title": "Excess risk bounds for multitask learning with trace norm regularization",
        "authors": [
          "M. Pontil",
          "A. Maurer"
        ],
        "year": 2013,
        "venue": "Proceedings of the Conference on Learning Theory, PMLR",
        "doi": null
      }
    },
    {
      "ref_id": "[33]",
      "parsed": {
        "title": "Generalization properties of learning with random features",
        "authors": [
          "A. Rudi",
          "L. Rosasco"
        ],
        "year": 2017,
        "venue": "Advances in Neural Information Processing Systems (NeurIPS)",
        "doi": null
      }
    },
    {
      "ref_id": "[34]",
      "parsed": {
        "title": "Variation spaces for multi-output neural networks: Insights on multi-task learning and network compression",
        "authors": [
          "J. Shenouda",
          "R. Parhi",
          "K. Lee",
          "R.D. Nowak"
        ],
        "year": 2024,
        "venue": "Journal of Machine Learning Research",
        "doi": null
      }
    },
    {
      "ref_id": "[35]",
      "parsed": {
        "title": "Scalable matrix-valued kernel learning for high-dimensional nonlinear multivariate regression and granger causality",
        "authors": [
          "V. Sindhwani",
          "H.Q. Minh",
          "A.C. Lozano"
        ],
        "year": 2013,
        "venue": "Proceedings of the 29th Conference on Uncertainty in Artificial Intelligence (UAI)",
        "doi": null
      }
    },
    {
      "ref_id": "[36]",
      "parsed": {
        "title": "Compression based bound for non-compressed network: unified generalization error analysis of large compressible deep neural network",
        "authors": [
          "T. Suzuki",
          "H. Abe",
          "T. Nishimura"
        ],
        "year": 2020,
        "venue": "Proceedings of the 8th International Conference on Learning Representations (ICLR)",
        "doi": null
      }
    },
    {
      "ref_id": "[37]",
      "parsed": {
        "title": "Approximation with matrix-valued kernels and highly effective error estimators for reduced basis approximations",
        "authors": [
          "D. Wittwar"
        ],
        "year": 2022,
        "venue": "Ph.D. thesis, Universität Stuttgart, Stuttgart, Germany",
        "doi": null
      }
    },
    {
      "ref_id": "[38]",
      "parsed": {
        "title": "Sketching as a tool for numerical linear algebra",
        "authors": [
          "D.P. Woodruff"
        ],
        "year": 2014,
        "venue": "Foundations and Trends® in Theoretical Computer Science",
        "doi": null
      }
    },
    {
      "ref_id": "[39]",
      "parsed": {
        "title": "Randomized sketches for kernels: Fast and optimal nonparametric regression",
        "authors": [
          "Y. Yang",
          "M. Pilanci",
          "M.J. Wainwright"
        ],
        "year": 2017,
        "venue": "The Annals of Statistics",
        "doi": null
      }
    },
    {
      "ref_id": "[40]",
      "parsed": {
        "title": "Local rademacher complexity-based learning guarantees for multi-task learning",
        "authors": [
          "N. Yousefi",
          "Y. Lei",
          "M. Kloft",
          "M. Mollaghasemi",
          "G.C. Anagnostopoulos"
        ],
        "year": 2018,
        "venue": "The Journal of Machine Learning Research",
        "doi": null
      }
    },
    {
      "ref_id": "[41]",
      "parsed": {
        "title": "Refinement of operator-valued reproducing kernels",
        "authors": [
          "H. Zhang",
          "Y. Xu",
          "Q. Zhang"
        ],
        "year": 2012,
        "venue": "Journal of Machine Learning Research",
        "doi": null
      }
    }
  ]
}